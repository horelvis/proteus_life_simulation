{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMHEIiPvXs6mDlGXhBvHMSs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/horelvis/proteus_life_simulation/blob/main/deeplearning-physics-word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CLASES FALTANTES PARA EL SISTEMA H√çBRIDO DE F√çSICA\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================================\n",
        "# 1. MODELO DE RED NEURONAL H√çBRIDA\n",
        "# ============================================================================\n",
        "\n",
        "class HybridPhysicsModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Red neuronal para aprender residuales de f√≠sica\n",
        "    Arquitectura optimizada para correcciones f√≠sicas\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int,\n",
        "                 dropout_rate: float = 0.1, activation: str = 'relu'):\n",
        "        super(HybridPhysicsModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Seleccionar funci√≥n de activaci√≥n\n",
        "        if activation.lower() == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation.lower() == 'leakyrelu':\n",
        "            self.activation = nn.LeakyReLU(0.1)\n",
        "        elif activation.lower() == 'gelu':\n",
        "            self.activation = nn.GELU()\n",
        "        elif activation.lower() == 'swish':\n",
        "            self.activation = nn.SiLU()  # SiLU es equivalente a Swish\n",
        "        else:\n",
        "            self.activation = nn.ReLU()\n",
        "\n",
        "        # Construir capas\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                self.activation,\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Capa de salida sin activaci√≥n (regresi√≥n)\n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "        # Inicializaci√≥n de pesos\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Inicializaci√≥n Xavier/Glorot para mejor convergencia\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass de la red\n",
        "\n",
        "        Args:\n",
        "            x: Tensor de entrada [batch_size, input_dim]\n",
        "\n",
        "        Returns:\n",
        "            Tensor de salida [batch_size, output_dim]\n",
        "        \"\"\"\n",
        "        return self.network(x)\n",
        "\n",
        "    def get_model_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"Informaci√≥n del modelo\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "        return {\n",
        "            'total_parameters': total_params,\n",
        "            'trainable_parameters': trainable_params,\n",
        "            'input_dim': self.input_dim,\n",
        "            'output_dim': self.output_dim,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'architecture': [layer for layer in self.network if isinstance(layer, nn.Linear)]\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ENTRENADOR OPTIMIZADO\n",
        "# ============================================================================\n",
        "\n",
        "class PhysicsTrainer:\n",
        "    \"\"\"\n",
        "    Entrenador especializado para modelos de f√≠sica h√≠brida\n",
        "    Incluye optimizaciones para GPU, mixed precision y logging\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
        "                 criterion: nn.Module, optimizer: optim.Optimizer,\n",
        "                 scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,\n",
        "                 device: torch.device = torch.device('cpu'),\n",
        "                 repo_id: str = \"physics-model\",\n",
        "                 use_mixed_precision: bool = True,\n",
        "                 use_cyclic_lr: bool = False,\n",
        "                 use_warmup: bool = True,\n",
        "                 warmup_epochs: int = 5):\n",
        "\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        self.repo_id = repo_id\n",
        "        self.use_mixed_precision = use_mixed_precision\n",
        "        self.use_cyclic_lr = use_cyclic_lr\n",
        "        self.use_warmup = use_warmup\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "\n",
        "        # Configurar mixed precision\n",
        "        self.scaler = None\n",
        "        if use_mixed_precision and torch.cuda.is_available():\n",
        "            try:\n",
        "                # Intentar usar la nueva API de torch.amp\n",
        "                from torch.amp import GradScaler, autocast\n",
        "                self.scaler = GradScaler('cuda')\n",
        "                self.autocast = autocast('cuda')\n",
        "                print(\"‚úÖ Using new API of mixed precision (torch.amp)\")\n",
        "            except ImportError:\n",
        "                try:\n",
        "                    # Fallback a la API antigua\n",
        "                    from torch.cuda.amp import GradScaler, autocast\n",
        "                    self.scaler = GradScaler()\n",
        "                    self.autocast = autocast()\n",
        "                    print(\"‚úÖ Using legacy API of mixed precision (torch.cuda.amp)\")\n",
        "                except ImportError:\n",
        "                    print(\"‚ö†Ô∏è Mixed precision not available, using standard precision\")\n",
        "                    self.use_mixed_precision = False\n",
        "        else:\n",
        "            self.use_mixed_precision = False\n",
        "\n",
        "        # Configurar learning rate scheduler c√≠clico\n",
        "        if use_cyclic_lr:\n",
        "            self.cyclic_scheduler = optim.lr_scheduler.CyclicLR(\n",
        "                optimizer, base_lr=1e-5, max_lr=1e-2,\n",
        "                step_size_up=len(train_loader) * 2,\n",
        "                mode='triangular2'\n",
        "            )\n",
        "\n",
        "        # Variables de tracking\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.learning_rates = []\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.epochs_without_improvement = 0\n",
        "\n",
        "        # Crear repositorio si es posible\n",
        "        self._setup_huggingface_repo()\n",
        "\n",
        "    def _setup_huggingface_repo(self):\n",
        "        \"\"\"Configurar repositorio de Hugging Face\"\"\"\n",
        "        try:\n",
        "            from huggingface_hub import HfApi, create_repo\n",
        "            self.hf_api = HfApi()\n",
        "\n",
        "            # Intentar crear el repositorio\n",
        "            try:\n",
        "                create_repo(self.repo_id, exist_ok=True, private=True)\n",
        "                print(f\"‚úÖ Hugging Face repository ready: {self.repo_id}\")\n",
        "                self.use_hf_upload = True\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Failed to create or check Hugging Face Hub repository: {e}\")\n",
        "                self.use_hf_upload = False\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è Hugging Face Hub not available\")\n",
        "            self.use_hf_upload = False\n",
        "\n",
        "    def _warmup_lr(self, epoch: int, initial_lr: float) -> float:\n",
        "        \"\"\"Calcula learning rate con warmup\"\"\"\n",
        "        if epoch < self.warmup_epochs:\n",
        "            return initial_lr * (epoch + 1) / self.warmup_epochs\n",
        "        return initial_lr\n",
        "\n",
        "    def train_epoch(self) -> float:\n",
        "        \"\"\"Entrena una √©poca\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            if self.use_mixed_precision and self.scaler is not None:\n",
        "                with self.autocast:\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = self.criterion(outputs, targets)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # Actualizar scheduler c√≠clico si est√° activado\n",
        "            if self.use_cyclic_lr:\n",
        "                self.cyclic_scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        return total_loss / num_batches\n",
        "\n",
        "    def validate(self) -> float:\n",
        "        \"\"\"Valida el modelo\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in self.val_loader:\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "                if self.use_mixed_precision and self.scaler is not None:\n",
        "                    with self.autocast:\n",
        "                        outputs = self.model(inputs)\n",
        "                        loss = self.criterion(outputs, targets)\n",
        "                else:\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = self.criterion(outputs, targets)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        return total_loss / num_batches\n",
        "\n",
        "    def save_model(self, filename: str = \"best_physics_model.pth\"):\n",
        "        \"\"\"Guarda el modelo localmente\"\"\"\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'val_loss': self.best_val_loss,\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "            'learning_rates': self.learning_rates\n",
        "        }, filename)\n",
        "        print(f\"  ‚úÖ Model saved locally ({filename}) (val_loss: {self.best_val_loss:.6f})\")\n",
        "\n",
        "    def upload_to_hub(self, filename: str = \"pytorch_model.bin\"):\n",
        "        \"\"\"Sube el modelo a Hugging Face Hub\"\"\"\n",
        "        if not self.use_hf_upload:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            from huggingface_hub import upload_file\n",
        "\n",
        "            # Guardar estado del modelo\n",
        "            temp_file = \"temp_model.bin\"\n",
        "            torch.save(self.model.state_dict(), temp_file)\n",
        "\n",
        "            # Subir a Hub\n",
        "            upload_file(\n",
        "                path_or_fileobj=temp_file,\n",
        "                path_in_repo=filename,\n",
        "                repo_id=self.repo_id,\n",
        "                repo_type=\"model\"\n",
        "            )\n",
        "\n",
        "            # Limpiar archivo temporal\n",
        "            if os.path.exists(temp_file):\n",
        "                os.remove(temp_file)\n",
        "\n",
        "            print(f\"  üöÄ Model uploaded to Hub: {self.repo_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Failed to upload to Hub: {e}\")\n",
        "\n",
        "    def train(self, num_epochs: int, early_stopping_patience: int = 10):\n",
        "        \"\"\"\n",
        "        Entrenamiento principal con todas las optimizaciones\n",
        "        \"\"\"\n",
        "        print(f\"üî• Starting training for {num_epochs} epochs...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Learning rate inicial para warmup\n",
        "        if self.use_warmup:\n",
        "            initial_lr = self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_start = time.time()\n",
        "\n",
        "            # Aplicar warmup si est√° habilitado\n",
        "            if self.use_warmup and epoch < self.warmup_epochs:\n",
        "                warmup_lr = self._warmup_lr(epoch, initial_lr)\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group['lr'] = warmup_lr\n",
        "\n",
        "            # Entrenar √©poca\n",
        "            train_loss = self.train_epoch()\n",
        "            val_loss = self.validate()\n",
        "\n",
        "            # Actualizar scheduler (no c√≠clico)\n",
        "            if self.scheduler and not self.use_cyclic_lr:\n",
        "                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                    self.scheduler.step(val_loss)\n",
        "                else:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            # Guardar m√©tricas\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            self.learning_rates.append(current_lr)\n",
        "\n",
        "            # Early stopping y guardado del mejor modelo\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.epochs_without_improvement = 0\n",
        "                self.save_model(\"best_physics_model_default.pth\")\n",
        "                # self.upload_to_hub()  # Comentado para evitar errores de autenticaci√≥n\n",
        "            else:\n",
        "                self.epochs_without_improvement += 1\n",
        "\n",
        "            # Informaci√≥n de √©poca\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            gpu_memory = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
        "\n",
        "            print(f\"\\nüìà Epoch {epoch+1}/{num_epochs}\")\n",
        "            if self.use_warmup and epoch < self.warmup_epochs:\n",
        "                print(f\"  Warmup LR: {current_lr:.6f}\")\n",
        "            print(f\"  Training Loss: {train_loss:.6f}\")\n",
        "            print(f\"  Validation Loss: {val_loss:.6f}\")\n",
        "            print(f\"  Time: {epoch_time:.2f}s\")\n",
        "            print(f\"  LR: {current_lr:.6f}\")\n",
        "            print(f\"  GPU Memory: {gpu_memory:.2f} GB\")\n",
        "\n",
        "            # Early stopping\n",
        "            if self.epochs_without_improvement >= early_stopping_patience:\n",
        "                print(f\"\\nüõë Early stopping triggered after {early_stopping_patience} epochs without improvement\")\n",
        "                break\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\nüéâ Training completed in {total_time:.2f}s\")\n",
        "        print(f\"üí´ Best validation loss: {self.best_val_loss:.6f}\")\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Visualiza el historial de entrenamiento\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # P√©rdidas\n",
        "        axes[0, 0].plot(self.train_losses, label='Training Loss', color='blue')\n",
        "        axes[0, 0].plot(self.val_losses, label='Validation Loss', color='red')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Training and Validation Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "\n",
        "        # Learning Rate\n",
        "        axes[0, 1].plot(self.learning_rates, color='green')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Learning Rate')\n",
        "        axes[0, 1].set_title('Learning Rate Schedule')\n",
        "        axes[0, 1].grid(True)\n",
        "        axes[0, 1].set_yscale('log')\n",
        "\n",
        "        # P√©rdida de validaci√≥n (zoom)\n",
        "        axes[1, 0].plot(self.val_losses, color='red')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Validation Loss')\n",
        "        axes[1, 0].set_title('Validation Loss (Detailed)')\n",
        "        axes[1, 0].grid(True)\n",
        "\n",
        "        # Diferencia entre train y val loss\n",
        "        if len(self.train_losses) == len(self.val_losses):\n",
        "            diff = [val - train for train, val in zip(self.train_losses, self.val_losses)]\n",
        "            axes[1, 1].plot(diff, color='purple')\n",
        "            axes[1, 1].set_xlabel('Epoch')\n",
        "            axes[1, 1].set_ylabel('Val Loss - Train Loss')\n",
        "            axes[1, 1].set_title('Overfitting Monitor')\n",
        "            axes[1, 1].grid(True)\n",
        "            axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 3. FUNCI√ìN DE P√âRDIDA ESPECIALIZADA (ya estaba definida pero la incluyo)\n",
        "# ============================================================================\n",
        "\n",
        "class PhysicsLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Funci√≥n de p√©rdida especializada para f√≠sica\n",
        "    Pondera diferentes componentes seg√∫n importancia f√≠sica\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, position_weight=1.0, velocity_weight=0.5):\n",
        "        super().__init__()\n",
        "        self.position_weight = position_weight\n",
        "        self.velocity_weight = velocity_weight\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        # Separar posici√≥n y velocidad\n",
        "        pred_pos = predictions[:, :3]\n",
        "        pred_vel = predictions[:, 3:]\n",
        "        target_pos = targets[:, :3]\n",
        "        target_vel = targets[:, 3:]\n",
        "\n",
        "        # P√©rdidas ponderadas\n",
        "        pos_loss = self.mse(pred_pos, target_pos)\n",
        "        vel_loss = self.mse(pred_vel, target_vel)\n",
        "\n",
        "        return self.position_weight * pos_loss + self.velocity_weight * vel_loss\n",
        "\n",
        "# ============================================================================\n",
        "# 4. UTILIDADES ADICIONALES\n",
        "# ============================================================================\n",
        "\n",
        "def create_model_from_config(config: Dict) -> HybridPhysicsModel:\n",
        "    \"\"\"\n",
        "    Crea un modelo a partir de una configuraci√≥n\n",
        "    \"\"\"\n",
        "    return HybridPhysicsModel(\n",
        "        input_dim=config['input_dim'],\n",
        "        hidden_dims=config['hidden_dims'],\n",
        "        output_dim=config['output_dim'],\n",
        "        dropout_rate=config.get('dropout_rate', 0.1),\n",
        "        activation=config.get('activation', 'relu')\n",
        "    )\n",
        "\n",
        "def load_trained_model(checkpoint_path: str, config: Dict, device: torch.device) -> HybridPhysicsModel:\n",
        "    \"\"\"\n",
        "    Carga un modelo entrenado desde un checkpoint\n",
        "    \"\"\"\n",
        "    model = create_model_from_config(config)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluador para m√©tricas adicionales del modelo\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, device: torch.device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def compute_metrics(self, data_loader: DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Computa m√©tricas detalladas en un dataset\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        total_mse = 0.0\n",
        "        total_mae = 0.0\n",
        "        total_position_error = 0.0\n",
        "        total_velocity_error = 0.0\n",
        "        num_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in data_loader:\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "                outputs = self.model(inputs)\n",
        "\n",
        "                # MSE y MAE generales\n",
        "                mse = F.mse_loss(outputs, targets, reduction='sum')\n",
        "                mae = F.l1_loss(outputs, targets, reduction='sum')\n",
        "\n",
        "                # Errores espec√≠ficos de posici√≥n y velocidad\n",
        "                pos_error = F.mse_loss(outputs[:, :3], targets[:, :3], reduction='sum')\n",
        "                vel_error = F.mse_loss(outputs[:, 3:], targets[:, 3:], reduction='sum')\n",
        "\n",
        "                total_mse += mse.item()\n",
        "                total_mae += mae.item()\n",
        "                total_position_error += pos_error.item()\n",
        "                total_velocity_error += vel_error.item()\n",
        "                num_samples += targets.size(0)\n",
        "\n",
        "        return {\n",
        "            'mse': total_mse / (num_samples * targets.size(1)),\n",
        "            'mae': total_mae / (num_samples * targets.size(1)),\n",
        "            'rmse': np.sqrt(total_mse / (num_samples * targets.size(1))),\n",
        "            'position_mse': total_position_error / (num_samples * 3),\n",
        "            'velocity_mse': total_velocity_error / (num_samples * 3),\n",
        "            'position_rmse': np.sqrt(total_position_error / (num_samples * 3)),\n",
        "            'velocity_rmse': np.sqrt(total_velocity_error / (num_samples * 3))\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# 5. CONFIGURACIONES DE EXPERIMENTOS\n",
        "# ============================================================================\n",
        "\n",
        "class ExperimentConfig:\n",
        "    \"\"\"\n",
        "    Configuraciones para diferentes experimentos\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_baseline_config() -> Dict:\n",
        "        \"\"\"Configuraci√≥n baseline\"\"\"\n",
        "        return {\n",
        "            'model': {\n",
        "                'hidden_dims': [512, 256, 128],\n",
        "                'dropout_rate': 0.1,\n",
        "                'activation': 'relu'\n",
        "            },\n",
        "            'training': {\n",
        "                'lr': 1e-3,\n",
        "                'weight_decay': 1e-4,\n",
        "                'batch_size': 256,\n",
        "                'epochs': 50,\n",
        "                'early_stopping_patience': 10\n",
        "            },\n",
        "            'loss': {\n",
        "                'position_weight': 1.0,\n",
        "                'velocity_weight': 0.5\n",
        "            }\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def get_advanced_config() -> Dict:\n",
        "        \"\"\"Configuraci√≥n avanzada\"\"\"\n",
        "        return {\n",
        "            'model': {\n",
        "                'hidden_dims': [1024, 512, 256, 128],\n",
        "                'dropout_rate': 0.2,\n",
        "                'activation': 'leakyrelu'\n",
        "            },\n",
        "            'training': {\n",
        "                'lr': 1e-3,\n",
        "                'weight_decay': 1e-4,\n",
        "                'batch_size': 512,\n",
        "                'epochs': 100,\n",
        "                'early_stopping_patience': 15,\n",
        "                'use_warmup': True,\n",
        "                'warmup_epochs': 5\n",
        "            },\n",
        "            'loss': {\n",
        "                'position_weight': 2.0,\n",
        "                'velocity_weight': 1.0\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Todas las clases faltantes han sido definidas correctamente\")\n",
        "print(\"üìã Clases disponibles:\")\n",
        "print(\"  ‚Ä¢ HybridPhysicsModel - Red neuronal h√≠brida\")\n",
        "print(\"  ‚Ä¢ PhysicsTrainer - Entrenador optimizado\")\n",
        "print(\"  ‚Ä¢ PhysicsLoss - Funci√≥n de p√©rdida especializada\")\n",
        "print(\"  ‚Ä¢ ModelEvaluator - Evaluador de m√©tricas\")\n",
        "print(\"  ‚Ä¢ ExperimentConfig - Configuraciones de experimentos\")\n",
        "print(\"  ‚Ä¢ Funciones auxiliares para carga y configuraci√≥n de modelos\")"
      ],
      "metadata": {
        "id": "ME5eKnJtqPJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CLASES ADICIONALES PARA SIMULACI√ìN Y MANEJO DE DATOS\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DATACLASS PARA PUNTOS DE DATOS (Refinada)\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class PhysicsDataPoint:\n",
        "    \"\"\"Data point extracted from real simulation - Version completa\"\"\"\n",
        "    basic_state: np.ndarray  # [pos_x, pos_y, pos_z, vel_x, vel_y, vel_z]\n",
        "    ground_truth_state: np.ndarray  # Real state from simulator\n",
        "    residual: np.ndarray  # Difference for IA to learn\n",
        "    material_properties: np.ndarray  # [obj_friction, obj_restitution, obj_damping, obj_density, floor_friction, floor_restitution, floor_damping, floor_density]\n",
        "    context: Dict  # Additional info (mass, shape, etc.)\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        \"\"\"Convierte a diccionario para serializaci√≥n\"\"\"\n",
        "        return {\n",
        "            'basic_state': self.basic_state.tolist(),\n",
        "            'ground_truth_state': self.ground_truth_state.tolist(),\n",
        "            'residual': self.residual.tolist(),\n",
        "            'material_properties': self.material_properties.tolist(),\n",
        "            'context': self.context\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict) -> 'PhysicsDataPoint':\n",
        "        \"\"\"Crea desde diccionario\"\"\"\n",
        "        return cls(\n",
        "            basic_state=np.array(data['basic_state']),\n",
        "            ground_truth_state=np.array(data['ground_truth_state']),\n",
        "            residual=np.array(data['residual']),\n",
        "            material_properties=np.array(data['material_properties']),\n",
        "            context=data['context']\n",
        "        )\n",
        "\n",
        "# ============================================================================\n",
        "# 2. DATASET PYTORCH OPTIMIZADO (Versi√≥n Completa)\n",
        "# ============================================================================\n",
        "\n",
        "class PhysicsDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset optimizado para entrenamiento en GPU - Versi√≥n completa\"\"\"\n",
        "\n",
        "    def __init__(self, data_points: List[PhysicsDataPoint],\n",
        "                 device: torch.device = None, normalize: bool = True,\n",
        "                 cache_in_memory: bool = True):\n",
        "        self.data_points = data_points\n",
        "        self.device = device or torch.device('cpu')\n",
        "        self.normalize = normalize\n",
        "        self.cache_in_memory = cache_in_memory\n",
        "\n",
        "        # Precomputar tensores en GPU para m√°ximo rendimiento\n",
        "        if cache_in_memory:\n",
        "            self._precompute_tensors()\n",
        "\n",
        "        if self.normalize:\n",
        "            self._compute_normalization_stats()\n",
        "\n",
        "    def _precompute_tensors(self):\n",
        "        \"\"\"Precompute all tensors and load them onto GPU\"\"\"\n",
        "        print(\"üî• Precomputing tensors on GPU...\")\n",
        "\n",
        "        # Extraer datos\n",
        "        basic_states = []\n",
        "        residuals = []\n",
        "        material_props = []\n",
        "\n",
        "        for dp in self.data_points:\n",
        "            basic_states.append(dp.basic_state)\n",
        "            residuals.append(dp.residual)\n",
        "            material_props.append(dp.material_properties)\n",
        "\n",
        "        # Convertir a tensores y mover a GPU\n",
        "        self.basic_states = torch.tensor(np.array(basic_states),\n",
        "                                       dtype=torch.float32, device=self.device)\n",
        "        self.residuals = torch.tensor(np.array(residuals),\n",
        "                                    dtype=torch.float32, device=self.device)\n",
        "        self.material_props = torch.tensor(np.array(material_props),\n",
        "                                         dtype=torch.float32, device=self.device)\n",
        "\n",
        "        print(f\"‚úÖ Tensors loaded on {self.device}\")\n",
        "        print(f\"  Basic states: {self.basic_states.shape}\")\n",
        "        print(f\"  Residuals: {self.residuals.shape}\")\n",
        "        print(f\"  Material properties: {self.material_props.shape}\")\n",
        "\n",
        "    def _compute_normalization_stats(self):\n",
        "        \"\"\"Compute statistics for normalization\"\"\"\n",
        "        if self.cache_in_memory:\n",
        "            # Normalizar basic states\n",
        "            self.state_mean = self.basic_states.mean(dim=0)\n",
        "            self.state_std = self.basic_states.std(dim=0) + 1e-8\n",
        "\n",
        "            # Normalizar material properties\n",
        "            self.mat_mean = self.material_props.mean(dim=0)\n",
        "            self.mat_std = self.material_props.std(dim=0) + 1e-8\n",
        "        else:\n",
        "            # Calcular estad√≠sticas sin cargar todo en memoria\n",
        "            basic_states = np.array([dp.basic_state for dp in self.data_points])\n",
        "            material_props = np.array([dp.material_properties for dp in self.data_points])\n",
        "\n",
        "            self.state_mean = torch.tensor(basic_states.mean(axis=0), dtype=torch.float32, device=self.device)\n",
        "            self.state_std = torch.tensor(basic_states.std(axis=0) + 1e-8, dtype=torch.float32, device=self.device)\n",
        "            self.mat_mean = torch.tensor(material_props.mean(axis=0), dtype=torch.float32, device=self.device)\n",
        "            self.mat_std = torch.tensor(material_props.std(axis=0) + 1e-8, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        print(\"üìä Normalization statistics computed\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_points)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get normalized sample\"\"\"\n",
        "        if self.cache_in_memory:\n",
        "            basic_state = self.basic_states[idx]\n",
        "            residual = self.residuals[idx]\n",
        "            material_prop = self.material_props[idx]\n",
        "        else:\n",
        "            # Cargar desde datos originales\n",
        "            dp = self.data_points[idx]\n",
        "            basic_state = torch.tensor(dp.basic_state, dtype=torch.float32, device=self.device)\n",
        "            residual = torch.tensor(dp.residual, dtype=torch.float32, device=self.device)\n",
        "            material_prop = torch.tensor(dp.material_properties, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        if self.normalize:\n",
        "            basic_state = (basic_state - self.state_mean) / self.state_std\n",
        "            material_prop = (material_prop - self.mat_mean) / self.mat_std\n",
        "\n",
        "        # Concatenate state + properties as input features\n",
        "        input_features = torch.cat([basic_state, material_prop])\n",
        "\n",
        "        return input_features, residual\n",
        "\n",
        "    def save_to_disk(self, filepath: str):\n",
        "        \"\"\"Guarda el dataset en disco\"\"\"\n",
        "        data_to_save = {\n",
        "            'data_points': [dp.to_dict() for dp in self.data_points],\n",
        "            'normalization_stats': {\n",
        "                'state_mean': self.state_mean.cpu().numpy() if hasattr(self, 'state_mean') else None,\n",
        "                'state_std': self.state_std.cpu().numpy() if hasattr(self, 'state_std') else None,\n",
        "                'mat_mean': self.mat_mean.cpu().numpy() if hasattr(self, 'mat_mean') else None,\n",
        "                'mat_std': self.mat_std.cpu().numpy() if hasattr(self, 'mat_std') else None,\n",
        "            },\n",
        "            'normalize': self.normalize,\n",
        "            'cache_in_memory': self.cache_in_memory\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(data_to_save, f)\n",
        "        print(f\"üíæ Dataset saved to {filepath}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_disk(cls, filepath: str, device: torch.device = None) -> 'PhysicsDataset':\n",
        "        \"\"\"Carga el dataset desde disco\"\"\"\n",
        "        with open(filepath, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        data_points = [PhysicsDataPoint.from_dict(dp_dict) for dp_dict in data['data_points']]\n",
        "\n",
        "        dataset = cls(\n",
        "            data_points=data_points,\n",
        "            device=device or torch.device('cpu'),\n",
        "            normalize=data['normalize'],\n",
        "            cache_in_memory=data['cache_in_memory']\n",
        "        )\n",
        "\n",
        "        # Restaurar estad√≠sticas de normalizaci√≥n si existen\n",
        "        if data['normalization_stats']['state_mean'] is not None:\n",
        "            dataset.state_mean = torch.tensor(data['normalization_stats']['state_mean'],\n",
        "                                            dtype=torch.float32, device=dataset.device)\n",
        "            dataset.state_std = torch.tensor(data['normalization_stats']['state_std'],\n",
        "                                           dtype=torch.float32, device=dataset.device)\n",
        "            dataset.mat_mean = torch.tensor(data['normalization_stats']['mat_mean'],\n",
        "                                          dtype=torch.float32, device=dataset.device)\n",
        "            dataset.mat_std = torch.tensor(data['normalization_stats']['mat_std'],\n",
        "                                         dtype=torch.float32, device=dataset.device)\n",
        "\n",
        "        print(f\"üìÇ Dataset loaded from {filepath}\")\n",
        "        return dataset\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ANALIZADOR DE DATOS\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetAnalyzer:\n",
        "    \"\"\"\n",
        "    Analizador para entender las caracter√≠sticas del dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset: PhysicsDataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def analyze_distribution(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analiza la distribuci√≥n de los datos\"\"\"\n",
        "\n",
        "        # Extraer datos para an√°lisis\n",
        "        basic_states = np.array([dp.basic_state for dp in self.dataset.data_points])\n",
        "        residuals = np.array([dp.residual for dp in self.dataset.data_points])\n",
        "        material_props = np.array([dp.material_properties for dp in self.dataset.data_points])\n",
        "\n",
        "        # An√°lisis de estados b√°sicos\n",
        "        pos_data = basic_states[:, :3]  # Posiciones\n",
        "        vel_data = basic_states[:, 3:]  # Velocidades\n",
        "\n",
        "        # An√°lisis de residuales\n",
        "        pos_residuals = residuals[:, :3]\n",
        "        vel_residuals = residuals[:, 3:]\n",
        "\n",
        "        analysis = {\n",
        "            'dataset_size': len(self.dataset.data_points),\n",
        "            'position_stats': {\n",
        "                'mean': np.mean(pos_data, axis=0),\n",
        "                'std': np.std(pos_data, axis=0),\n",
        "                'min': np.min(pos_data, axis=0),\n",
        "                'max': np.max(pos_data, axis=0)\n",
        "            },\n",
        "            'velocity_stats': {\n",
        "                'mean': np.mean(vel_data, axis=0),\n",
        "                'std': np.std(vel_data, axis=0),\n",
        "                'min': np.min(vel_data, axis=0),\n",
        "                'max': np.max(vel_data, axis=0)\n",
        "            },\n",
        "            'position_residual_stats': {\n",
        "                'mean': np.mean(pos_residuals, axis=0),\n",
        "                'std': np.std(pos_residuals, axis=0),\n",
        "                'min': np.min(pos_residuals, axis=0),\n",
        "                'max': np.max(pos_residuals, axis=0)\n",
        "            },\n",
        "            'velocity_residual_stats': {\n",
        "                'mean': np.mean(vel_residuals, axis=0),\n",
        "                'std': np.std(vel_residuals, axis=0),\n",
        "                'min': np.min(vel_residuals, axis=0),\n",
        "                'max': np.max(vel_residuals, axis=0)\n",
        "            },\n",
        "            'material_distribution': self._analyze_materials()\n",
        "        }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _analyze_materials(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analiza la distribuci√≥n de materiales\"\"\"\n",
        "        materials = {}\n",
        "        floor_materials = {}\n",
        "        shapes = {}\n",
        "\n",
        "        for dp in self.dataset.data_points:\n",
        "            # Material del objeto\n",
        "            obj_material = dp.context.get('material', 'unknown')\n",
        "            materials[obj_material] = materials.get(obj_material, 0) + 1\n",
        "\n",
        "            # Material del suelo\n",
        "            floor_material = dp.context.get('floor_material', 'unknown')\n",
        "            floor_materials[floor_material] = floor_materials.get(floor_material, 0) + 1\n",
        "\n",
        "            # Forma del objeto\n",
        "            shape = dp.context.get('shape', 'unknown')\n",
        "            shapes[shape] = shapes.get(shape, 0) + 1\n",
        "\n",
        "        return {\n",
        "            'object_materials': materials,\n",
        "            'floor_materials': floor_materials,\n",
        "            'shapes': shapes\n",
        "        }\n",
        "\n",
        "    def plot_analysis(self):\n",
        "        \"\"\"Visualiza el an√°lisis del dataset\"\"\"\n",
        "        analysis = self.analyze_distribution()\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # Distribuci√≥n de posiciones\n",
        "        pos_data = np.array([dp.basic_state[:3] for dp in self.dataset.data_points])\n",
        "        for i, axis_name in enumerate(['X', 'Y', 'Z']):\n",
        "            axes[0, i].hist(pos_data[:, i], bins=50, alpha=0.7, color=['red', 'green', 'blue'][i])\n",
        "            axes[0, i].set_title(f'Position {axis_name} Distribution')\n",
        "            axes[0, i].set_xlabel(f'Position {axis_name}')\n",
        "            axes[0, i].set_ylabel('Frequency')\n",
        "            axes[0, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Distribuci√≥n de velocidades\n",
        "        vel_data = np.array([dp.basic_state[3:] for dp in self.dataset.data_points])\n",
        "        for i, axis_name in enumerate(['VX', 'VY', 'VZ']):\n",
        "            axes[1, i].hist(vel_data[:, i], bins=50, alpha=0.7, color=['orange', 'purple', 'brown'][i])\n",
        "            axes[1, i].set_title(f'Velocity {axis_name} Distribution')\n",
        "            axes[1, i].set_xlabel(f'Velocity {axis_name}')\n",
        "            axes[1, i].set_ylabel('Frequency')\n",
        "            axes[1, i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Gr√°fico de distribuci√≥n de materiales\n",
        "        self._plot_material_distribution(analysis['material_distribution'])\n",
        "\n",
        "    def _plot_material_distribution(self, material_dist: Dict):\n",
        "        \"\"\"Grafica la distribuci√≥n de materiales\"\"\"\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        # Materiales de objetos\n",
        "        if material_dist['object_materials']:\n",
        "            materials = list(material_dist['object_materials'].keys())\n",
        "            counts = list(material_dist['object_materials'].values())\n",
        "            axes[0].pie(counts, labels=materials, autopct='%1.1f%%')\n",
        "            axes[0].set_title('Object Materials Distribution')\n",
        "\n",
        "        # Materiales de suelo\n",
        "        if material_dist['floor_materials']:\n",
        "            floor_materials = list(material_dist['floor_materials'].keys())\n",
        "            floor_counts = list(material_dist['floor_materials'].values())\n",
        "            axes[1].pie(floor_counts, labels=floor_materials, autopct='%1.1f%%')\n",
        "            axes[1].set_title('Floor Materials Distribution')\n",
        "\n",
        "        # Formas\n",
        "        if material_dist['shapes']:\n",
        "            shapes = list(material_dist['shapes'].keys())\n",
        "            shape_counts = list(material_dist['shapes'].values())\n",
        "            axes[2].pie(shape_counts, labels=shapes, autopct='%1.1f%%')\n",
        "            axes[2].set_title('Shapes Distribution')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 4. GENERADOR DE DATOS SINT√âTICOS\n",
        "# ============================================================================\n",
        "\n",
        "class SyntheticDataGenerator:\n",
        "    \"\"\"\n",
        "    Generador de datos sint√©ticos para aumentar el dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset: PhysicsDataset):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.analyzer = DatasetAnalyzer(base_dataset)\n",
        "        self.stats = self.analyzer.analyze_distribution()\n",
        "\n",
        "    def generate_synthetic_data(self, num_samples: int,\n",
        "                              noise_level: float = 0.1) -> List[PhysicsDataPoint]:\n",
        "        \"\"\"\n",
        "        Genera datos sint√©ticos basados en el dataset existente\n",
        "        \"\"\"\n",
        "        synthetic_data = []\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            # Seleccionar un punto base aleatorio\n",
        "            base_idx = random.randint(0, len(self.base_dataset.data_points) - 1)\n",
        "            base_point = self.base_dataset.data_points[base_idx]\n",
        "\n",
        "            # Agregar ruido controlado\n",
        "            noisy_basic_state = self._add_noise(base_point.basic_state, noise_level)\n",
        "            noisy_material_props = self._add_noise(base_point.material_properties, noise_level * 0.1)\n",
        "\n",
        "            # Generar residual sint√©tico (con cierta correlaci√≥n)\n",
        "            synthetic_residual = self._generate_correlated_residual(\n",
        "                noisy_basic_state, base_point.residual, noise_level\n",
        "            )\n",
        "\n",
        "            # Crear nuevo punto de datos\n",
        "            synthetic_point = PhysicsDataPoint(\n",
        "                basic_state=noisy_basic_state,\n",
        "                ground_truth_state=noisy_basic_state + synthetic_residual,\n",
        "                residual=synthetic_residual,\n",
        "                material_properties=noisy_material_props,\n",
        "                context={**base_point.context, 'synthetic': True}\n",
        "            )\n",
        "\n",
        "            synthetic_data.append(synthetic_point)\n",
        "\n",
        "        return synthetic_data\n",
        "\n",
        "    def _add_noise(self, data: np.ndarray, noise_level: float) -> np.ndarray:\n",
        "        \"\"\"Agrega ruido gaussiano a los datos\"\"\"\n",
        "        noise = np.random.normal(0, noise_level, data.shape)\n",
        "        return data + noise * np.std(data, axis=0 if data.ndim > 1 else None)\n",
        "\n",
        "    def _generate_correlated_residual(self, basic_state: np.ndarray,\n",
        "                                    base_residual: np.ndarray,\n",
        "                                    noise_level: float) -> np.ndarray:\n",
        "        \"\"\"Genera residual sint√©tico con correlaci√≥n f√≠sica\"\"\"\n",
        "        # Mantener cierta correlaci√≥n con el residual base\n",
        "        correlation_factor = 0.7\n",
        "        noise_factor = 1 - correlation_factor\n",
        "\n",
        "        noise = np.random.normal(0, noise_level, base_residual.shape)\n",
        "        synthetic_residual = (correlation_factor * base_residual +\n",
        "                            noise_factor * noise * np.std(base_residual))\n",
        "\n",
        "        return synthetic_residual\n",
        "\n",
        "# ============================================================================\n",
        "# 5. UTILIDADES DE VISUALIZACI√ìN\n",
        "# ============================================================================\n",
        "\n",
        "class PhysicsVisualizer:\n",
        "    \"\"\"\n",
        "    Herramientas de visualizaci√≥n para datos de f√≠sica\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_trajectory_3d(states: List[Dict], title: str = \"3D Trajectory\"):\n",
        "        \"\"\"Visualiza una trayectoria en 3D\"\"\"\n",
        "        positions = np.array([state['position'] for state in states])\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 8))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        # Trayectoria\n",
        "        ax.plot(positions[:, 0], positions[:, 1], positions[:, 2],\n",
        "                'b-', linewidth=2, label='Trajectory')\n",
        "\n",
        "        # Punto inicial\n",
        "        ax.scatter(positions[0, 0], positions[0, 1], positions[0, 2],\n",
        "                  color='green', s=100, label='Start')\n",
        "\n",
        "        # Punto final\n",
        "        ax.scatter(positions[-1, 0], positions[-1, 1], positions[-1, 2],\n",
        "                  color='red', s=100, label='End')\n",
        "\n",
        "        ax.set_xlabel('X')\n",
        "        ax.set_ylabel('Y')\n",
        "        ax.set_zlabel('Z')\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_residual_analysis(dataset: PhysicsDataset):\n",
        "        \"\"\"Analiza y visualiza los residuales\"\"\"\n",
        "        residuals = np.array([dp.residual for dp in dataset.data_points])\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "        # Residuales de posici√≥n\n",
        "        pos_residuals = residuals[:, :3]\n",
        "        for i, axis in enumerate(['X', 'Y', 'Z']):\n",
        "            axes[0, i].hist(pos_residuals[:, i], bins=50, alpha=0.7)\n",
        "            axes[0, i].set_title(f'Position Residual {axis}')\n",
        "            axes[0, i].set_xlabel(f'Residual {axis}')\n",
        "            axes[0, i].set_ylabel('Frequency')\n",
        "            axes[0, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Residuales de velocidad\n",
        "        vel_residuals = residuals[:, 3:]\n",
        "        for i, axis in enumerate(['VX', 'VY', 'VZ']):\n",
        "            axes[1, i].hist(vel_residuals[:, i], bins=50, alpha=0.7)\n",
        "            axes[1, i].set_title(f'Velocity Residual {axis}')\n",
        "            axes[1, i].set_xlabel(f'Residual V{axis[1]}')\n",
        "            axes[1, i].set_ylabel('Frequency')\n",
        "            axes[1, i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# MENSAJE DE CONFIRMACI√ìN\n",
        "# ============================================================================\n",
        "\n",
        "print(\"‚úÖ Clases adicionales de simulaci√≥n y datos completadas:\")\n",
        "print(\"  ‚Ä¢ PhysicsDataPoint (completa) - Punto de datos con serializaci√≥n\")\n",
        "print(\"  ‚Ä¢ PhysicsDataset (completa) - Dataset optimizado con cache y persistencia\")\n",
        "print(\"  ‚Ä¢ DatasetAnalyzer - Analizador de distribuciones de datos\")\n",
        "print(\"  ‚Ä¢ SyntheticDataGenerator - Generador de datos sint√©ticos\")\n",
        "print(\"  ‚Ä¢ PhysicsVisualizer - Herramientas de visualizaci√≥n\")\n",
        "print(\"\\nüîß Funcionalidades adicionales:\")\n",
        "print(\"  ‚Ä¢ Guardado y carga de datasets\")\n",
        "print(\"  ‚Ä¢ An√°lisis estad√≠stico completo\")\n",
        "print(\"  ‚Ä¢ Generaci√≥n de datos sint√©ticos\")\n",
        "print(\"  ‚Ä¢ Visualizaciones especializadas\")"
      ],
      "metadata": {
        "id": "goWKhNUFtSqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MAIN PIPELINE - SISTEMA H√çBRIDO DE F√çSICA CON IA\n",
        "# ============================================================================\n",
        "# Este script orquesta todo el flujo del sistema h√≠brido de f√≠sica\n",
        "\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importar todas las clases definidas anteriormente\n",
        "# (Asumiendo que est√°n en el mismo notebook o importadas)\n",
        "\n",
        "class HybridPhysicsExperiment:\n",
        "    \"\"\"\n",
        "    Clase principal que orquesta todo el experimento de f√≠sica h√≠brida\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.results = {}\n",
        "\n",
        "        # Directorios de trabajo\n",
        "        self.setup_directories()\n",
        "\n",
        "        print(f\"üöÄ Iniciando Experimento de F√≠sica H√≠brida\")\n",
        "        print(f\"üì± Dispositivo: {self.device}\")\n",
        "        print(f\"üîß Configuraci√≥n: {config['experiment_name']}\")\n",
        "\n",
        "    def setup_directories(self):\n",
        "        \"\"\"Configura los directorios de trabajo\"\"\"\n",
        "        self.base_dir = Path(self.config.get('output_dir', 'hybrid_physics_output'))\n",
        "        self.base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        self.models_dir = self.base_dir / 'models'\n",
        "        self.data_dir = self.base_dir / 'data'\n",
        "        self.plots_dir = self.base_dir / 'plots'\n",
        "        self.logs_dir = self.base_dir / 'logs'\n",
        "\n",
        "        for dir_path in [self.models_dir, self.data_dir, self.plots_dir, self.logs_dir]:\n",
        "            dir_path.mkdir(exist_ok=True)\n",
        "\n",
        "    def run_full_pipeline(self):\n",
        "        \"\"\"Ejecuta el pipeline completo\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üéØ EJECUTANDO PIPELINE COMPLETO\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        try:\n",
        "            # Paso 1: Generar datos si no existen\n",
        "            dataset = self.step_1_generate_or_load_data()\n",
        "\n",
        "            # Paso 2: Analizar datos\n",
        "            self.step_2_analyze_data(dataset)\n",
        "\n",
        "            # Paso 3: Preparar datos para entrenamiento\n",
        "            train_loader, val_loader, test_loader = self.step_3_prepare_data_loaders(dataset)\n",
        "\n",
        "            # Paso 4: Crear y entrenar modelo\n",
        "            model, trainer = self.step_4_train_model(train_loader, val_loader)\n",
        "\n",
        "            # Paso 5: Evaluar modelo\n",
        "            self.step_5_evaluate_model(model, test_loader, dataset)\n",
        "\n",
        "            # Paso 6: Crear sistema h√≠brido y validar\n",
        "            self.step_6_validate_hybrid_system(model, dataset)\n",
        "\n",
        "            # Paso 7: Generar reporte final\n",
        "            self.step_7_generate_final_report()\n",
        "\n",
        "            print(\"\\nüéâ ¬°PIPELINE COMPLETADO EXITOSAMENTE!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error en el pipeline: {e}\")\n",
        "            raise\n",
        "\n",
        "    def step_1_generate_or_load_data(self):\n",
        "        \"\"\"Paso 1: Generar o cargar datos\"\"\"\n",
        "        print(\"\\nüìä PASO 1: Generaci√≥n/Carga de Datos\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        dataset_path = self.data_dir / 'physics_dataset.pkl'\n",
        "\n",
        "        if dataset_path.exists() and not self.config.get('regenerate_data', False):\n",
        "            print(\"üìÇ Cargando dataset existente...\")\n",
        "            dataset = PhysicsDataset.load_from_disk(str(dataset_path), self.device)\n",
        "        else:\n",
        "            print(\"üîÑ Generando nuevo dataset...\")\n",
        "\n",
        "            # Configurar generador de datos\n",
        "            generator_config = self.config.get('data_generation', {})\n",
        "            num_scenarios = generator_config.get('num_scenarios', 1000)\n",
        "            steps_per_scenario = generator_config.get('steps_per_scenario', 100)\n",
        "\n",
        "            # Generar datos\n",
        "            data_generator = RealPhysicsDataGenerator(\n",
        "                num_scenarios=num_scenarios,\n",
        "                steps_per_scenario=steps_per_scenario\n",
        "            )\n",
        "\n",
        "            physics_dataset = data_generator.generate_dataset()\n",
        "\n",
        "            # Crear dataset PyTorch\n",
        "            dataset = PhysicsDataset(\n",
        "                physics_dataset,\n",
        "                device=self.device,\n",
        "                normalize=True\n",
        "            )\n",
        "\n",
        "            # Guardar dataset\n",
        "            dataset.save_to_disk(str(dataset_path))\n",
        "\n",
        "        print(f\"‚úÖ Dataset listo: {len(dataset)} muestras\")\n",
        "        self.results['dataset_size'] = len(dataset)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def step_2_analyze_data(self, dataset):\n",
        "        \"\"\"Paso 2: Analizar distribuci√≥n de datos\"\"\"\n",
        "        print(\"\\nüîç PASO 2: An√°lisis de Datos\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        analyzer = DatasetAnalyzer(dataset)\n",
        "        analysis = analyzer.analyze_distribution()\n",
        "\n",
        "        # Guardar an√°lisis\n",
        "        analysis_path = self.logs_dir / 'data_analysis.json'\n",
        "        with open(analysis_path, 'w') as f:\n",
        "            # Convertir numpy arrays a listas para JSON\n",
        "            def convert_numpy(obj):\n",
        "                if isinstance(obj, np.ndarray):\n",
        "                    return obj.tolist()\n",
        "                elif isinstance(obj, np.integer):\n",
        "                    return int(obj)\n",
        "                elif isinstance(obj, np.floating):\n",
        "                    return float(obj)\n",
        "                elif isinstance(obj, dict):\n",
        "                    return {key: convert_numpy(value) for key, value in obj.items()}\n",
        "                elif isinstance(obj, list):\n",
        "                    return [convert_numpy(item) for item in obj]\n",
        "                return obj\n",
        "\n",
        "            json.dump(convert_numpy(analysis), f, indent=2)\n",
        "\n",
        "        # Generar visualizaciones\n",
        "        print(\"üìà Generando visualizaciones...\")\n",
        "        analyzer.plot_analysis()\n",
        "        plt.savefig(self.plots_dir / 'data_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        PhysicsVisualizer.plot_residual_analysis(dataset)\n",
        "        plt.savefig(self.plots_dir / 'residual_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"‚úÖ An√°lisis completado\")\n",
        "        self.results['data_analysis'] = analysis\n",
        "\n",
        "    def step_3_prepare_data_loaders(self, dataset):\n",
        "        \"\"\"Paso 3: Preparar data loaders\"\"\"\n",
        "        print(\"\\nüîÑ PASO 3: Preparaci√≥n de Data Loaders\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Configuraci√≥n de divisi√≥n de datos\n",
        "        data_config = self.config.get('data_split', {})\n",
        "        train_ratio = data_config.get('train_ratio', 0.7)\n",
        "        val_ratio = data_config.get('val_ratio', 0.2)\n",
        "        test_ratio = data_config.get('test_ratio', 0.1)\n",
        "\n",
        "        # Verificar que las proporciones suman 1\n",
        "        total_ratio = train_ratio + val_ratio + test_ratio\n",
        "        if abs(total_ratio - 1.0) > 1e-6:\n",
        "            raise ValueError(f\"Las proporciones deben sumar 1.0, pero suman {total_ratio}\")\n",
        "\n",
        "        # Calcular tama√±os\n",
        "        dataset_size = len(dataset)\n",
        "        train_size = int(train_ratio * dataset_size)\n",
        "        val_size = int(val_ratio * dataset_size)\n",
        "        test_size = dataset_size - train_size - val_size\n",
        "\n",
        "        # Dividir dataset\n",
        "        train_dataset, val_dataset, test_dataset = random_split(\n",
        "            dataset, [train_size, val_size, test_size],\n",
        "            generator=torch.Generator().manual_seed(42)\n",
        "        )\n",
        "\n",
        "        # Configurar data loaders\n",
        "        batch_size = self.config.get('training', {}).get('batch_size', 512)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=0, pin_memory=False\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=0, pin_memory=False\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=0, pin_memory=False\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Data loaders creados:\")\n",
        "        print(f\"  üìö Entrenamiento: {len(train_dataset)} muestras\")\n",
        "        print(f\"  üîç Validaci√≥n: {len(val_dataset)} muestras\")\n",
        "        print(f\"  üß™ Prueba: {len(test_dataset)} muestras\")\n",
        "        print(f\"  üì¶ Batch size: {batch_size}\")\n",
        "\n",
        "        self.results['data_split'] = {\n",
        "            'train_size': len(train_dataset),\n",
        "            'val_size': len(val_dataset),\n",
        "            'test_size': len(test_dataset),\n",
        "            'batch_size': batch_size\n",
        "        }\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "    def step_4_train_model(self, train_loader, val_loader):\n",
        "        \"\"\"Paso 4: Entrenar modelo\"\"\"\n",
        "        print(\"\\nüß† PASO 4: Entrenamiento del Modelo\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Configuraci√≥n del modelo\n",
        "        model_config = self.config.get('model', {})\n",
        "\n",
        "        # Determinar dimensiones\n",
        "        sample_input, sample_output = next(iter(train_loader))\n",
        "        input_dim = sample_input.shape[1]\n",
        "        output_dim = sample_output.shape[1]\n",
        "\n",
        "        # Crear modelo\n",
        "        model = HybridPhysicsModel(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=model_config.get('hidden_dims', [1024, 512, 256, 128]),\n",
        "            output_dim=output_dim,\n",
        "            dropout_rate=model_config.get('dropout_rate', 0.2),\n",
        "            activation=model_config.get('activation', 'leakyrelu')\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Configurar entrenamiento\n",
        "        training_config = self.config.get('training', {})\n",
        "\n",
        "        # Optimizador\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=training_config.get('lr', 1e-3),\n",
        "            weight_decay=training_config.get('weight_decay', 1e-4)\n",
        "        )\n",
        "\n",
        "        # Scheduler\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "\n",
        "        # Funci√≥n de p√©rdida\n",
        "        loss_config = self.config.get('loss', {})\n",
        "        criterion = PhysicsLoss(\n",
        "            position_weight=loss_config.get('position_weight', 2.0),\n",
        "            velocity_weight=loss_config.get('velocity_weight', 1.0)\n",
        "        )\n",
        "\n",
        "        # Crear entrenador\n",
        "        trainer = PhysicsTrainer(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            device=self.device,\n",
        "            repo_id=f\"hybrid-physics-{self.config['experiment_name']}\",\n",
        "            use_warmup=training_config.get('use_warmup', True),\n",
        "            warmup_epochs=training_config.get('warmup_epochs', 5)\n",
        "        )\n",
        "\n",
        "        # Entrenar\n",
        "        num_epochs = training_config.get('epochs', 30)\n",
        "        early_stopping_patience = training_config.get('early_stopping_patience', 8)\n",
        "\n",
        "        print(f\"üî• Iniciando entrenamiento por {num_epochs} √©pocas...\")\n",
        "        trainer.train(num_epochs=num_epochs, early_stopping_patience=early_stopping_patience)\n",
        "\n",
        "        # Guardar modelo\n",
        "        model_path = self.models_dir / 'best_hybrid_model.pth'\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'model_config': {\n",
        "                'input_dim': input_dim,\n",
        "                'hidden_dims': model_config.get('hidden_dims', [1024, 512, 256, 128]),\n",
        "                'output_dim': output_dim,\n",
        "                'dropout_rate': model_config.get('dropout_rate', 0.2),\n",
        "                'activation': model_config.get('activation', 'leakyrelu')\n",
        "            },\n",
        "            'training_history': {\n",
        "                'train_losses': trainer.train_losses,\n",
        "                'val_losses': trainer.val_losses,\n",
        "                'learning_rates': trainer.learning_rates,\n",
        "                'best_val_loss': trainer.best_val_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "\n",
        "        # Visualizar historial de entrenamiento\n",
        "        trainer.plot_training_history()\n",
        "        plt.savefig(self.plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"‚úÖ Entrenamiento completado\")\n",
        "        self.results['training'] = {\n",
        "            'best_val_loss': trainer.best_val_loss,\n",
        "            'final_train_loss': trainer.train_losses[-1] if trainer.train_losses else None,\n",
        "            'total_epochs': len(trainer.train_losses),\n",
        "            'model_parameters': sum(p.numel() for p in model.parameters())\n",
        "        }\n",
        "\n",
        "        return model, trainer\n",
        "\n",
        "    def step_5_evaluate_model(self, model, test_loader, dataset):\n",
        "        \"\"\"Paso 5: Evaluar modelo en conjunto de prueba\"\"\"\n",
        "        print(\"\\nüìä PASO 5: Evaluaci√≥n del Modelo\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        evaluator = ModelEvaluator(model, self.device)\n",
        "        metrics = evaluator.compute_metrics(test_loader)\n",
        "\n",
        "        print(\"üìà M√©tricas en conjunto de prueba:\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            print(f\"  {metric_name}: {value:.6f}\")\n",
        "\n",
        "        # Guardar m√©tricas\n",
        "        metrics_path = self.logs_dir / 'test_metrics.json'\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "\n",
        "        self.results['test_metrics'] = metrics\n",
        "        print(\"‚úÖ Evaluaci√≥n completada\")\n",
        "\n",
        "    def step_6_validate_hybrid_system(self, model, dataset):\n",
        "        \"\"\"Paso 6: Validar sistema h√≠brido completo\"\"\"\n",
        "        print(\"\\nüî¨ PASO 6: Validaci√≥n del Sistema H√≠brido\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Crear sistema h√≠brido\n",
        "        basic_physics_core = NewtonianPhysics()\n",
        "\n",
        "        normalization_stats = {\n",
        "            'state_mean': dataset.state_mean,\n",
        "            'state_std': dataset.state_std,\n",
        "            'mat_mean': dataset.mat_mean,\n",
        "            'mat_std': dataset.mat_std\n",
        "        }\n",
        "\n",
        "        hybrid_system = RealHybridPhysicsSystem(\n",
        "            trained_model=model,\n",
        "            physics_core=basic_physics_core,\n",
        "            normalization_stats=normalization_stats\n",
        "        )\n",
        "\n",
        "        # Crear simulador de alta fidelidad\n",
        "        high_fidelity_sim = HighFidelitySimulator(gui=False)\n",
        "\n",
        "        # Crear validador\n",
        "        validator = PhysicsValidator(\n",
        "            hybrid_system=hybrid_system,\n",
        "            basic_physics=basic_physics_core,\n",
        "            high_fidelity_sim=high_fidelity_sim\n",
        "        )\n",
        "\n",
        "        # Crear escenarios de prueba\n",
        "        validation_config = self.config.get('validation', {})\n",
        "        num_test_scenarios = validation_config.get('num_scenarios', 5)\n",
        "        num_steps = validation_config.get('num_steps', 50)\n",
        "\n",
        "        print(f\"üß™ Creando {num_test_scenarios} escenarios de prueba...\")\n",
        "        test_scenarios = create_test_scenarios(num_scenarios=num_test_scenarios)\n",
        "\n",
        "        # Ejecutar validaci√≥n\n",
        "        print(f\"‚ö° Ejecutando validaci√≥n con {num_steps} pasos por escenario...\")\n",
        "        validation_results = validator.run_comparison_test(test_scenarios, num_steps=num_steps)\n",
        "\n",
        "        # Analizar resultados\n",
        "        stats, improvement = validator.analyze_results(validation_results)\n",
        "\n",
        "        # Visualizar comparaci√≥n para el primer escenario\n",
        "        validator.plot_comparison(validation_results, scenario_idx=0)\n",
        "        plt.savefig(self.plots_dir / 'hybrid_validation.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Guardar resultados de validaci√≥n\n",
        "        validation_summary = {\n",
        "            'hybrid_stats': stats['hybrid'],\n",
        "            'basic_stats': stats['basic'],\n",
        "            'improvement': improvement,\n",
        "            'num_scenarios': num_test_scenarios,\n",
        "            'steps_per_scenario': num_steps\n",
        "        }\n",
        "\n",
        "        validation_path = self.logs_dir / 'validation_results.json'\n",
        "        with open(validation_path, 'w') as f:\n",
        "            json.dump(validation_summary, f, indent=2)\n",
        "\n",
        "        print(\"‚úÖ Validaci√≥n del sistema h√≠brido completada\")\n",
        "        self.results['validation'] = validation_summary\n",
        "\n",
        "    def step_7_generate_final_report(self):\n",
        "        \"\"\"Paso 7: Generar reporte final\"\"\"\n",
        "        print(\"\\nüìÑ PASO 7: Generaci√≥n de Reporte Final\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Crear reporte completo\n",
        "        report = self._create_final_report()\n",
        "\n",
        "        # Guardar reporte\n",
        "        report_path = self.base_dir / 'REPORTE_FINAL.md'\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        # Guardar resultados completos en JSON\n",
        "        results_path = self.logs_dir / 'experiment_results.json'\n",
        "        with open(results_path, 'w') as f:\n",
        "            json.dump(self.results, f, indent=2)\n",
        "\n",
        "        print(f\"üìä Reporte final guardado en: {report_path}\")\n",
        "        print(f\"üíæ Resultados completos en: {results_path}\")\n",
        "        print(\"‚úÖ Reporte generado exitosamente\")\n",
        "\n",
        "    def _create_final_report(self) -> str:\n",
        "        \"\"\"Crea el reporte final en formato Markdown\"\"\"\n",
        "\n",
        "        validation = self.results.get('validation', {})\n",
        "        training = self.results.get('training', {})\n",
        "\n",
        "        report = f\"\"\"# üöÄ REPORTE FINAL - SISTEMA H√çBRIDO DE F√çSICA CON IA\n",
        "\n",
        "## üìã Informaci√≥n del Experimento\n",
        "- **Nombre del Experimento**: {self.config['experiment_name']}\n",
        "- **Fecha de Ejecuci√≥n**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- **Dispositivo**: {self.device}\n",
        "\n",
        "## üìä Resumen de Datos\n",
        "- **Tama√±o del Dataset**: {self.results.get('dataset_size', 'N/A')} muestras\n",
        "- **Divisi√≥n de Datos**:\n",
        "  - Entrenamiento: {self.results.get('data_split', {}).get('train_size', 'N/A')} muestras\n",
        "  - Validaci√≥n: {self.results.get('data_split', {}).get('val_size', 'N/A')} muestras\n",
        "  - Prueba: {self.results.get('data_split', {}).get('test_size', 'N/A')} muestras\n",
        "\n",
        "## üß† Configuraci√≥n del Modelo\n",
        "- **Par√°metros del Modelo**: {training.get('model_parameters', 'N/A'):,} par√°metros\n",
        "- **Arquitectura**: {self.config.get('model', {}).get('hidden_dims', 'N/A')}\n",
        "- **Funci√≥n de Activaci√≥n**: {self.config.get('model', {}).get('activation', 'N/A')}\n",
        "\n",
        "## üìà Resultados del Entrenamiento\n",
        "- **Mejor P√©rdida de Validaci√≥n**: {training.get('best_val_loss', 'N/A'):.6f}\n",
        "- **P√©rdida Final de Entrenamiento**: {training.get('final_train_loss', 'N/A'):.6f}\n",
        "- **√âpocas Completadas**: {training.get('total_epochs', 'N/A')}\n",
        "\n",
        "## üéØ Rendimiento del Sistema H√≠brido\n",
        "- **Mejora en Precisi√≥n de Posici√≥n**: {validation.get('improvement', {}).get('position_error_reduction', 'N/A'):.1f}%\n",
        "- **Mejora en Precisi√≥n de Velocidad**: {validation.get('improvement', {}).get('velocity_error_reduction', 'N/A'):.1f}%\n",
        "\n",
        "### M√©tricas Detalladas:\n",
        "#### Sistema H√≠brido:\n",
        "- Error promedio posici√≥n: {validation.get('hybrid_stats', {}).get('mean_pos_error', 'N/A'):.6f}\n",
        "- Error promedio velocidad: {validation.get('hybrid_stats', {}).get('mean_vel_error', 'N/A'):.6f}\n",
        "\n",
        "#### F√≠sica B√°sica:\n",
        "- Error promedio posici√≥n: {validation.get('basic_stats', {}).get('mean_pos_error', 'N/A'):.6f}\n",
        "- Error promedio velocidad: {validation.get('basic_stats', {}).get('mean_vel_error', 'N/A'):.6f}\n",
        "\n",
        "## üî¨ Innovaciones T√©cnicas\n",
        "1. **F√≠sica B√°sica Mejorada**: Inclusi√≥n de fricci√≥n est√°tica/din√°mica y resistencia del aire\n",
        "2. **Dataset Realista**: Generaci√≥n autom√°tica con m√∫ltiples materiales y condiciones\n",
        "3. **Arquitectura H√≠brida**: Combinaci√≥n √≥ptima de f√≠sica determinista + correcci√≥n por IA\n",
        "4. **Entrenamiento Optimizado**: Mixed precision, warmup, early stopping\n",
        "5. **Validaci√≥n Rigurosa**: Comparaci√≥n directa con simulador de alta fidelidad PyBullet\n",
        "\n",
        "## üìÅ Archivos Generados\n",
        "- `models/best_hybrid_model.pth` - Modelo entrenado\n",
        "- `data/physics_dataset.pkl` - Dataset generado\n",
        "- `plots/` - Visualizaciones y gr√°ficos\n",
        "- `logs/` - M√©tricas y an√°lisis detallados\n",
        "\n",
        "## üöÄ Aplicaciones Potenciales\n",
        "- Simulaciones de videojuegos en tiempo real\n",
        "- Sistemas de entrenamiento rob√≥tico\n",
        "- Predicci√≥n de movimiento de objetos f√≠sicos\n",
        "- Simulaciones cient√≠ficas aceleradas\n",
        "\n",
        "## üìà Pr√≥ximos Pasos\n",
        "- Expansi√≥n a sistemas multi-objeto\n",
        "- Inclusi√≥n de deformaciones y rotaciones complejas\n",
        "- Optimizaci√≥n para inferencia en tiempo real\n",
        "- Integraci√≥n con motores de f√≠sica comerciales\n",
        "\n",
        "---\n",
        "*Generado autom√°ticamente por el Sistema H√≠brido de F√≠sica con IA*\n",
        "\"\"\"\n",
        "        return report\n",
        "\n",
        "\n",
        "def create_default_config() -> Dict[str, Any]:\n",
        "    \"\"\"Crea configuraci√≥n por defecto para el experimento\"\"\"\n",
        "    return {\n",
        "        'experiment_name': 'hybrid_physics_default',\n",
        "        'output_dir': 'hybrid_physics_output',\n",
        "        'regenerate_data': False,\n",
        "\n",
        "        'data_generation': {\n",
        "            'num_scenarios': 1000,\n",
        "            'steps_per_scenario': 100\n",
        "        },\n",
        "\n",
        "        'data_split': {\n",
        "            'train_ratio': 0.7,\n",
        "            'val_ratio': 0.2,\n",
        "            'test_ratio': 0.1\n",
        "        },\n",
        "\n",
        "        'model': {\n",
        "            'hidden_dims': [1024, 512, 256, 128],\n",
        "            'dropout_rate': 0.2,\n",
        "            'activation': 'leakyrelu'\n",
        "        },\n",
        "\n",
        "        'training': {\n",
        "            'batch_size': 512,\n",
        "            'lr': 1e-3,\n",
        "            'weight_decay': 1e-4,\n",
        "            'epochs': 30,\n",
        "            'early_stopping_patience': 8,\n",
        "            'use_warmup': True,\n",
        "            'warmup_epochs': 5\n",
        "        },\n",
        "\n",
        "        'loss': {\n",
        "            'position_weight': 2.0,\n",
        "            'velocity_weight': 1.0\n",
        "        },\n",
        "\n",
        "        'validation': {\n",
        "            'num_scenarios': 5,\n",
        "            'num_steps': 50\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Funci√≥n principal\"\"\"\n",
        "    print(\"üåü SISTEMA H√çBRIDO DE F√çSICA CON IA\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Configuraci√≥n por defecto\n",
        "    config = create_default_config()\n",
        "\n",
        "    # Crear y ejecutar experimento\n",
        "    experiment = HybridPhysicsExperiment(config)\n",
        "    experiment.run_full_pipeline()\n",
        "\n",
        "    print(\"\\nüéä ¬°EXPERIMENTO COMPLETADO EXITOSAMENTE!\")\n",
        "    print(f\"üìÇ Resultados guardados en: {experiment.base_dir}\")\n",
        "\n",
        "\n",
        "def main_with_custom_config(config_path: str):\n",
        "    \"\"\"Funci√≥n principal con configuraci√≥n personalizada\"\"\"\n",
        "    print(\"üåü SISTEMA H√çBRIDO DE F√çSICA CON IA (Configuraci√≥n Personalizada)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Cargar configuraci√≥n personalizada\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Crear y ejecutar experimento\n",
        "    experiment = HybridPhysicsExperiment(config)\n",
        "    experiment.run_full_pipeline()\n",
        "\n",
        "    print(\"\\nüéä ¬°EXPERIMENTO COMPLETADO EXITOSAMENTE!\")\n",
        "    print(f\"üìÇ Resultados guardados en: {experiment.base_dir}\")\n",
        "\n",
        "\n",
        "def create_sample_config():\n",
        "    \"\"\"Crea un archivo de configuraci√≥n de ejemplo\"\"\"\n",
        "    config = create_default_config()\n",
        "\n",
        "    # Modificar para experimento de ejemplo\n",
        "    config['experiment_name'] = 'physics_experiment_sample'\n",
        "    config['data_generation']['num_scenarios'] = 500  # Menos datos para prueba r√°pida\n",
        "    config['training']['epochs'] = 15  # Menos √©pocas para prueba r√°pida\n",
        "\n",
        "    config_path = 'sample_config.json'\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"üìÑ Configuraci√≥n de ejemplo creada: {config_path}\")\n",
        "    return config_path\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PUNTO DE ENTRADA PRINCIPAL\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Si se ejecuta directamente, usar configuraci√≥n por defecto\n",
        "    main()\n",
        "\n",
        "# Para usar en Jupyter Notebook:\n",
        "def run_experiment(custom_config: Optional[Dict] = None):\n",
        "    \"\"\"\n",
        "    Funci√≥n de conveniencia para ejecutar desde Jupyter Notebook\n",
        "\n",
        "    Args:\n",
        "        custom_config: Configuraci√≥n personalizada (opcional)\n",
        "    \"\"\"\n",
        "    if custom_config is None:\n",
        "        config = create_default_config()\n",
        "    else:\n",
        "        config = custom_config\n",
        "\n",
        "    experiment = HybridPhysicsExperiment(config)\n",
        "    experiment.run_full_pipeline()\n",
        "\n",
        "    return experiment\n",
        "\n",
        "# ============================================================================\n",
        "# EJEMPLOS DE USO\n",
        "# ============================================================================\n",
        "\n",
        "def example_quick_test():\n",
        "    \"\"\"Ejemplo para prueba r√°pida\"\"\"\n",
        "    config = create_default_config()\n",
        "    config['experiment_name'] = 'quick_test'\n",
        "    config['data_generation']['num_scenarios'] = 100\n",
        "    config['training']['epochs'] = 10\n",
        "    config['validation']['num_scenarios'] = 3\n",
        "\n",
        "    return run_experiment(config)\n",
        "\n",
        "def example_full_experiment():\n",
        "    \"\"\"Ejemplo para experimento completo\"\"\"\n",
        "    config = create_default_config()\n",
        "    config['experiment_name'] = 'full_physics_experiment'\n",
        "    config['data_generation']['num_scenarios'] = 2000\n",
        "    config['training']['epochs'] = 50\n",
        "\n",
        "    return run_experiment(config)\n",
        "\n",
        "print(\"‚úÖ Main pipeline definido correctamente\")\n",
        "print(\"\\nüéØ Para ejecutar el experimento completo, usa:\")\n",
        "print(\"  ‚Ä¢ main() - Ejecuta con configuraci√≥n por defecto\")\n",
        "print(\"  ‚Ä¢ run_experiment() - Para usar en Jupyter Notebook\")\n",
        "print(\"  ‚Ä¢ example_quick_test() - Prueba r√°pida\")\n",
        "print(\"  ‚Ä¢ example_full_experiment() - Experimento completo\")\n",
        "print(\"\\nüìã Todas las funciones est√°n listas para usar!\")"
      ],
      "metadata": {
        "id": "9HOjXyoLuAwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "n6FAw1fAFKk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}