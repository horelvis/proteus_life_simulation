{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMHEIiPvXs6mDlGXhBvHMSs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/horelvis/proteus_life_simulation/blob/main/deeplearning-physics-word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CLASES FALTANTES PARA EL SISTEMA HÍBRIDO DE FÍSICA\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================================\n",
        "# 1. MODELO DE RED NEURONAL HÍBRIDA\n",
        "# ============================================================================\n",
        "\n",
        "class HybridPhysicsModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Red neuronal para aprender residuales de física\n",
        "    Arquitectura optimizada para correcciones físicas\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int,\n",
        "                 dropout_rate: float = 0.1, activation: str = 'relu'):\n",
        "        super(HybridPhysicsModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Seleccionar función de activación\n",
        "        if activation.lower() == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation.lower() == 'leakyrelu':\n",
        "            self.activation = nn.LeakyReLU(0.1)\n",
        "        elif activation.lower() == 'gelu':\n",
        "            self.activation = nn.GELU()\n",
        "        elif activation.lower() == 'swish':\n",
        "            self.activation = nn.SiLU()  # SiLU es equivalente a Swish\n",
        "        else:\n",
        "            self.activation = nn.ReLU()\n",
        "\n",
        "        # Construir capas\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                self.activation,\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Capa de salida sin activación (regresión)\n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "        # Inicialización de pesos\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Inicialización Xavier/Glorot para mejor convergencia\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass de la red\n",
        "\n",
        "        Args:\n",
        "            x: Tensor de entrada [batch_size, input_dim]\n",
        "\n",
        "        Returns:\n",
        "            Tensor de salida [batch_size, output_dim]\n",
        "        \"\"\"\n",
        "        return self.network(x)\n",
        "\n",
        "    def get_model_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"Información del modelo\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "        return {\n",
        "            'total_parameters': total_params,\n",
        "            'trainable_parameters': trainable_params,\n",
        "            'input_dim': self.input_dim,\n",
        "            'output_dim': self.output_dim,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'architecture': [layer for layer in self.network if isinstance(layer, nn.Linear)]\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ENTRENADOR OPTIMIZADO\n",
        "# ============================================================================\n",
        "\n",
        "class PhysicsTrainer:\n",
        "    \"\"\"\n",
        "    Entrenador especializado para modelos de física híbrida\n",
        "    Incluye optimizaciones para GPU, mixed precision y logging\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
        "                 criterion: nn.Module, optimizer: optim.Optimizer,\n",
        "                 scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,\n",
        "                 device: torch.device = torch.device('cpu'),\n",
        "                 repo_id: str = \"physics-model\",\n",
        "                 use_mixed_precision: bool = True,\n",
        "                 use_cyclic_lr: bool = False,\n",
        "                 use_warmup: bool = True,\n",
        "                 warmup_epochs: int = 5):\n",
        "\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        self.repo_id = repo_id\n",
        "        self.use_mixed_precision = use_mixed_precision\n",
        "        self.use_cyclic_lr = use_cyclic_lr\n",
        "        self.use_warmup = use_warmup\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "\n",
        "        # Configurar mixed precision\n",
        "        self.scaler = None\n",
        "        if use_mixed_precision and torch.cuda.is_available():\n",
        "            try:\n",
        "                # Intentar usar la nueva API de torch.amp\n",
        "                from torch.amp import GradScaler, autocast\n",
        "                self.scaler = GradScaler('cuda')\n",
        "                self.autocast = autocast('cuda')\n",
        "                print(\"✅ Using new API of mixed precision (torch.amp)\")\n",
        "            except ImportError:\n",
        "                try:\n",
        "                    # Fallback a la API antigua\n",
        "                    from torch.cuda.amp import GradScaler, autocast\n",
        "                    self.scaler = GradScaler()\n",
        "                    self.autocast = autocast()\n",
        "                    print(\"✅ Using legacy API of mixed precision (torch.cuda.amp)\")\n",
        "                except ImportError:\n",
        "                    print(\"⚠️ Mixed precision not available, using standard precision\")\n",
        "                    self.use_mixed_precision = False\n",
        "        else:\n",
        "            self.use_mixed_precision = False\n",
        "\n",
        "        # Configurar learning rate scheduler cíclico\n",
        "        if use_cyclic_lr:\n",
        "            self.cyclic_scheduler = optim.lr_scheduler.CyclicLR(\n",
        "                optimizer, base_lr=1e-5, max_lr=1e-2,\n",
        "                step_size_up=len(train_loader) * 2,\n",
        "                mode='triangular2'\n",
        "            )\n",
        "\n",
        "        # Variables de tracking\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.learning_rates = []\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.epochs_without_improvement = 0\n",
        "\n",
        "        # Crear repositorio si es posible\n",
        "        self._setup_huggingface_repo()\n",
        "\n",
        "    def _setup_huggingface_repo(self):\n",
        "        \"\"\"Configurar repositorio de Hugging Face\"\"\"\n",
        "        try:\n",
        "            from huggingface_hub import HfApi, create_repo\n",
        "            self.hf_api = HfApi()\n",
        "\n",
        "            # Intentar crear el repositorio\n",
        "            try:\n",
        "                create_repo(self.repo_id, exist_ok=True, private=True)\n",
        "                print(f\"✅ Hugging Face repository ready: {self.repo_id}\")\n",
        "                self.use_hf_upload = True\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Failed to create or check Hugging Face Hub repository: {e}\")\n",
        "                self.use_hf_upload = False\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"⚠️ Hugging Face Hub not available\")\n",
        "            self.use_hf_upload = False\n",
        "\n",
        "    def _warmup_lr(self, epoch: int, initial_lr: float) -> float:\n",
        "        \"\"\"Calcula learning rate con warmup\"\"\"\n",
        "        if epoch < self.warmup_epochs:\n",
        "            return initial_lr * (epoch + 1) / self.warmup_epochs\n",
        "        return initial_lr\n",
        "\n",
        "    def train_epoch(self) -> float:\n",
        "        \"\"\"Entrena una época\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            if self.use_mixed_precision and self.scaler is not None:\n",
        "                with self.autocast:\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = self.criterion(outputs, targets)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # Actualizar scheduler cíclico si está activado\n",
        "            if self.use_cyclic_lr:\n",
        "                self.cyclic_scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        return total_loss / num_batches\n",
        "\n",
        "    def validate(self) -> float:\n",
        "        \"\"\"Valida el modelo\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in self.val_loader:\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "                if self.use_mixed_precision and self.scaler is not None:\n",
        "                    with self.autocast:\n",
        "                        outputs = self.model(inputs)\n",
        "                        loss = self.criterion(outputs, targets)\n",
        "                else:\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = self.criterion(outputs, targets)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        return total_loss / num_batches\n",
        "\n",
        "    def save_model(self, filename: str = \"best_physics_model.pth\"):\n",
        "        \"\"\"Guarda el modelo localmente\"\"\"\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'val_loss': self.best_val_loss,\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "            'learning_rates': self.learning_rates\n",
        "        }, filename)\n",
        "        print(f\"  ✅ Model saved locally ({filename}) (val_loss: {self.best_val_loss:.6f})\")\n",
        "\n",
        "    def upload_to_hub(self, filename: str = \"pytorch_model.bin\"):\n",
        "        \"\"\"Sube el modelo a Hugging Face Hub\"\"\"\n",
        "        if not self.use_hf_upload:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            from huggingface_hub import upload_file\n",
        "\n",
        "            # Guardar estado del modelo\n",
        "            temp_file = \"temp_model.bin\"\n",
        "            torch.save(self.model.state_dict(), temp_file)\n",
        "\n",
        "            # Subir a Hub\n",
        "            upload_file(\n",
        "                path_or_fileobj=temp_file,\n",
        "                path_in_repo=filename,\n",
        "                repo_id=self.repo_id,\n",
        "                repo_type=\"model\"\n",
        "            )\n",
        "\n",
        "            # Limpiar archivo temporal\n",
        "            if os.path.exists(temp_file):\n",
        "                os.remove(temp_file)\n",
        "\n",
        "            print(f\"  🚀 Model uploaded to Hub: {self.repo_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️ Failed to upload to Hub: {e}\")\n",
        "\n",
        "    def train(self, num_epochs: int, early_stopping_patience: int = 10):\n",
        "        \"\"\"\n",
        "        Entrenamiento principal con todas las optimizaciones\n",
        "        \"\"\"\n",
        "        print(f\"🔥 Starting training for {num_epochs} epochs...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Learning rate inicial para warmup\n",
        "        if self.use_warmup:\n",
        "            initial_lr = self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_start = time.time()\n",
        "\n",
        "            # Aplicar warmup si está habilitado\n",
        "            if self.use_warmup and epoch < self.warmup_epochs:\n",
        "                warmup_lr = self._warmup_lr(epoch, initial_lr)\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group['lr'] = warmup_lr\n",
        "\n",
        "            # Entrenar época\n",
        "            train_loss = self.train_epoch()\n",
        "            val_loss = self.validate()\n",
        "\n",
        "            # Actualizar scheduler (no cíclico)\n",
        "            if self.scheduler and not self.use_cyclic_lr:\n",
        "                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                    self.scheduler.step(val_loss)\n",
        "                else:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            # Guardar métricas\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            self.learning_rates.append(current_lr)\n",
        "\n",
        "            # Early stopping y guardado del mejor modelo\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.epochs_without_improvement = 0\n",
        "                self.save_model(\"best_physics_model_default.pth\")\n",
        "                # self.upload_to_hub()  # Comentado para evitar errores de autenticación\n",
        "            else:\n",
        "                self.epochs_without_improvement += 1\n",
        "\n",
        "            # Información de época\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            gpu_memory = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
        "\n",
        "            print(f\"\\n📈 Epoch {epoch+1}/{num_epochs}\")\n",
        "            if self.use_warmup and epoch < self.warmup_epochs:\n",
        "                print(f\"  Warmup LR: {current_lr:.6f}\")\n",
        "            print(f\"  Training Loss: {train_loss:.6f}\")\n",
        "            print(f\"  Validation Loss: {val_loss:.6f}\")\n",
        "            print(f\"  Time: {epoch_time:.2f}s\")\n",
        "            print(f\"  LR: {current_lr:.6f}\")\n",
        "            print(f\"  GPU Memory: {gpu_memory:.2f} GB\")\n",
        "\n",
        "            # Early stopping\n",
        "            if self.epochs_without_improvement >= early_stopping_patience:\n",
        "                print(f\"\\n🛑 Early stopping triggered after {early_stopping_patience} epochs without improvement\")\n",
        "                break\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\n🎉 Training completed in {total_time:.2f}s\")\n",
        "        print(f\"💫 Best validation loss: {self.best_val_loss:.6f}\")\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Visualiza el historial de entrenamiento\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Pérdidas\n",
        "        axes[0, 0].plot(self.train_losses, label='Training Loss', color='blue')\n",
        "        axes[0, 0].plot(self.val_losses, label='Validation Loss', color='red')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Training and Validation Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "\n",
        "        # Learning Rate\n",
        "        axes[0, 1].plot(self.learning_rates, color='green')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Learning Rate')\n",
        "        axes[0, 1].set_title('Learning Rate Schedule')\n",
        "        axes[0, 1].grid(True)\n",
        "        axes[0, 1].set_yscale('log')\n",
        "\n",
        "        # Pérdida de validación (zoom)\n",
        "        axes[1, 0].plot(self.val_losses, color='red')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Validation Loss')\n",
        "        axes[1, 0].set_title('Validation Loss (Detailed)')\n",
        "        axes[1, 0].grid(True)\n",
        "\n",
        "        # Diferencia entre train y val loss\n",
        "        if len(self.train_losses) == len(self.val_losses):\n",
        "            diff = [val - train for train, val in zip(self.train_losses, self.val_losses)]\n",
        "            axes[1, 1].plot(diff, color='purple')\n",
        "            axes[1, 1].set_xlabel('Epoch')\n",
        "            axes[1, 1].set_ylabel('Val Loss - Train Loss')\n",
        "            axes[1, 1].set_title('Overfitting Monitor')\n",
        "            axes[1, 1].grid(True)\n",
        "            axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 3. FUNCIÓN DE PÉRDIDA ESPECIALIZADA (ya estaba definida pero la incluyo)\n",
        "# ============================================================================\n",
        "\n",
        "class PhysicsLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Función de pérdida especializada para física\n",
        "    Pondera diferentes componentes según importancia física\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, position_weight=1.0, velocity_weight=0.5):\n",
        "        super().__init__()\n",
        "        self.position_weight = position_weight\n",
        "        self.velocity_weight = velocity_weight\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        # Separar posición y velocidad\n",
        "        pred_pos = predictions[:, :3]\n",
        "        pred_vel = predictions[:, 3:]\n",
        "        target_pos = targets[:, :3]\n",
        "        target_vel = targets[:, 3:]\n",
        "\n",
        "        # Pérdidas ponderadas\n",
        "        pos_loss = self.mse(pred_pos, target_pos)\n",
        "        vel_loss = self.mse(pred_vel, target_vel)\n",
        "\n",
        "        return self.position_weight * pos_loss + self.velocity_weight * vel_loss\n",
        "\n",
        "# ============================================================================\n",
        "# 4. UTILIDADES ADICIONALES\n",
        "# ============================================================================\n",
        "\n",
        "def create_model_from_config(config: Dict) -> HybridPhysicsModel:\n",
        "    \"\"\"\n",
        "    Crea un modelo a partir de una configuración\n",
        "    \"\"\"\n",
        "    return HybridPhysicsModel(\n",
        "        input_dim=config['input_dim'],\n",
        "        hidden_dims=config['hidden_dims'],\n",
        "        output_dim=config['output_dim'],\n",
        "        dropout_rate=config.get('dropout_rate', 0.1),\n",
        "        activation=config.get('activation', 'relu')\n",
        "    )\n",
        "\n",
        "def load_trained_model(checkpoint_path: str, config: Dict, device: torch.device) -> HybridPhysicsModel:\n",
        "    \"\"\"\n",
        "    Carga un modelo entrenado desde un checkpoint\n",
        "    \"\"\"\n",
        "    model = create_model_from_config(config)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluador para métricas adicionales del modelo\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, device: torch.device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def compute_metrics(self, data_loader: DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Computa métricas detalladas en un dataset\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        total_mse = 0.0\n",
        "        total_mae = 0.0\n",
        "        total_position_error = 0.0\n",
        "        total_velocity_error = 0.0\n",
        "        num_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in data_loader:\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "                outputs = self.model(inputs)\n",
        "\n",
        "                # MSE y MAE generales\n",
        "                mse = F.mse_loss(outputs, targets, reduction='sum')\n",
        "                mae = F.l1_loss(outputs, targets, reduction='sum')\n",
        "\n",
        "                # Errores específicos de posición y velocidad\n",
        "                pos_error = F.mse_loss(outputs[:, :3], targets[:, :3], reduction='sum')\n",
        "                vel_error = F.mse_loss(outputs[:, 3:], targets[:, 3:], reduction='sum')\n",
        "\n",
        "                total_mse += mse.item()\n",
        "                total_mae += mae.item()\n",
        "                total_position_error += pos_error.item()\n",
        "                total_velocity_error += vel_error.item()\n",
        "                num_samples += targets.size(0)\n",
        "\n",
        "        return {\n",
        "            'mse': total_mse / (num_samples * targets.size(1)),\n",
        "            'mae': total_mae / (num_samples * targets.size(1)),\n",
        "            'rmse': np.sqrt(total_mse / (num_samples * targets.size(1))),\n",
        "            'position_mse': total_position_error / (num_samples * 3),\n",
        "            'velocity_mse': total_velocity_error / (num_samples * 3),\n",
        "            'position_rmse': np.sqrt(total_position_error / (num_samples * 3)),\n",
        "            'velocity_rmse': np.sqrt(total_velocity_error / (num_samples * 3))\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# 5. CONFIGURACIONES DE EXPERIMENTOS\n",
        "# ============================================================================\n",
        "\n",
        "class ExperimentConfig:\n",
        "    \"\"\"\n",
        "    Configuraciones para diferentes experimentos\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_baseline_config() -> Dict:\n",
        "        \"\"\"Configuración baseline\"\"\"\n",
        "        return {\n",
        "            'model': {\n",
        "                'hidden_dims': [512, 256, 128],\n",
        "                'dropout_rate': 0.1,\n",
        "                'activation': 'relu'\n",
        "            },\n",
        "            'training': {\n",
        "                'lr': 1e-3,\n",
        "                'weight_decay': 1e-4,\n",
        "                'batch_size': 256,\n",
        "                'epochs': 50,\n",
        "                'early_stopping_patience': 10\n",
        "            },\n",
        "            'loss': {\n",
        "                'position_weight': 1.0,\n",
        "                'velocity_weight': 0.5\n",
        "            }\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def get_advanced_config() -> Dict:\n",
        "        \"\"\"Configuración avanzada\"\"\"\n",
        "        return {\n",
        "            'model': {\n",
        "                'hidden_dims': [1024, 512, 256, 128],\n",
        "                'dropout_rate': 0.2,\n",
        "                'activation': 'leakyrelu'\n",
        "            },\n",
        "            'training': {\n",
        "                'lr': 1e-3,\n",
        "                'weight_decay': 1e-4,\n",
        "                'batch_size': 512,\n",
        "                'epochs': 100,\n",
        "                'early_stopping_patience': 15,\n",
        "                'use_warmup': True,\n",
        "                'warmup_epochs': 5\n",
        "            },\n",
        "            'loss': {\n",
        "                'position_weight': 2.0,\n",
        "                'velocity_weight': 1.0\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(\"✅ Todas las clases faltantes han sido definidas correctamente\")\n",
        "print(\"📋 Clases disponibles:\")\n",
        "print(\"  • HybridPhysicsModel - Red neuronal híbrida\")\n",
        "print(\"  • PhysicsTrainer - Entrenador optimizado\")\n",
        "print(\"  • PhysicsLoss - Función de pérdida especializada\")\n",
        "print(\"  • ModelEvaluator - Evaluador de métricas\")\n",
        "print(\"  • ExperimentConfig - Configuraciones de experimentos\")\n",
        "print(\"  • Funciones auxiliares para carga y configuración de modelos\")"
      ],
      "metadata": {
        "id": "ME5eKnJtqPJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CLASES ADICIONALES PARA SIMULACIÓN Y MANEJO DE DATOS\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DATACLASS PARA PUNTOS DE DATOS (Refinada)\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class PhysicsDataPoint:\n",
        "    \"\"\"Data point extracted from real simulation - Version completa\"\"\"\n",
        "    basic_state: np.ndarray  # [pos_x, pos_y, pos_z, vel_x, vel_y, vel_z]\n",
        "    ground_truth_state: np.ndarray  # Real state from simulator\n",
        "    residual: np.ndarray  # Difference for IA to learn\n",
        "    material_properties: np.ndarray  # [obj_friction, obj_restitution, obj_damping, obj_density, floor_friction, floor_restitution, floor_damping, floor_density]\n",
        "    context: Dict  # Additional info (mass, shape, etc.)\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        \"\"\"Convierte a diccionario para serialización\"\"\"\n",
        "        return {\n",
        "            'basic_state': self.basic_state.tolist(),\n",
        "            'ground_truth_state': self.ground_truth_state.tolist(),\n",
        "            'residual': self.residual.tolist(),\n",
        "            'material_properties': self.material_properties.tolist(),\n",
        "            'context': self.context\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict) -> 'PhysicsDataPoint':\n",
        "        \"\"\"Crea desde diccionario\"\"\"\n",
        "        return cls(\n",
        "            basic_state=np.array(data['basic_state']),\n",
        "            ground_truth_state=np.array(data['ground_truth_state']),\n",
        "            residual=np.array(data['residual']),\n",
        "            material_properties=np.array(data['material_properties']),\n",
        "            context=data['context']\n",
        "        )\n",
        "\n",
        "# ============================================================================\n",
        "# 2. DATASET PYTORCH OPTIMIZADO (Versión Completa)\n",
        "# ============================================================================\n",
        "\n",
        "class PhysicsDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset optimizado para entrenamiento en GPU - Versión completa\"\"\"\n",
        "\n",
        "    def __init__(self, data_points: List[PhysicsDataPoint],\n",
        "                 device: torch.device = None, normalize: bool = True,\n",
        "                 cache_in_memory: bool = True):\n",
        "        self.data_points = data_points\n",
        "        self.device = device or torch.device('cpu')\n",
        "        self.normalize = normalize\n",
        "        self.cache_in_memory = cache_in_memory\n",
        "\n",
        "        # Precomputar tensores en GPU para máximo rendimiento\n",
        "        if cache_in_memory:\n",
        "            self._precompute_tensors()\n",
        "\n",
        "        if self.normalize:\n",
        "            self._compute_normalization_stats()\n",
        "\n",
        "    def _precompute_tensors(self):\n",
        "        \"\"\"Precompute all tensors and load them onto GPU\"\"\"\n",
        "        print(\"🔥 Precomputing tensors on GPU...\")\n",
        "\n",
        "        # Extraer datos\n",
        "        basic_states = []\n",
        "        residuals = []\n",
        "        material_props = []\n",
        "\n",
        "        for dp in self.data_points:\n",
        "            basic_states.append(dp.basic_state)\n",
        "            residuals.append(dp.residual)\n",
        "            material_props.append(dp.material_properties)\n",
        "\n",
        "        # Convertir a tensores y mover a GPU\n",
        "        self.basic_states = torch.tensor(np.array(basic_states),\n",
        "                                       dtype=torch.float32, device=self.device)\n",
        "        self.residuals = torch.tensor(np.array(residuals),\n",
        "                                    dtype=torch.float32, device=self.device)\n",
        "        self.material_props = torch.tensor(np.array(material_props),\n",
        "                                         dtype=torch.float32, device=self.device)\n",
        "\n",
        "        print(f\"✅ Tensors loaded on {self.device}\")\n",
        "        print(f\"  Basic states: {self.basic_states.shape}\")\n",
        "        print(f\"  Residuals: {self.residuals.shape}\")\n",
        "        print(f\"  Material properties: {self.material_props.shape}\")\n",
        "\n",
        "    def _compute_normalization_stats(self):\n",
        "        \"\"\"Compute statistics for normalization\"\"\"\n",
        "        if self.cache_in_memory:\n",
        "            # Normalizar basic states\n",
        "            self.state_mean = self.basic_states.mean(dim=0)\n",
        "            self.state_std = self.basic_states.std(dim=0) + 1e-8\n",
        "\n",
        "            # Normalizar material properties\n",
        "            self.mat_mean = self.material_props.mean(dim=0)\n",
        "            self.mat_std = self.material_props.std(dim=0) + 1e-8\n",
        "        else:\n",
        "            # Calcular estadísticas sin cargar todo en memoria\n",
        "            basic_states = np.array([dp.basic_state for dp in self.data_points])\n",
        "            material_props = np.array([dp.material_properties for dp in self.data_points])\n",
        "\n",
        "            self.state_mean = torch.tensor(basic_states.mean(axis=0), dtype=torch.float32, device=self.device)\n",
        "            self.state_std = torch.tensor(basic_states.std(axis=0) + 1e-8, dtype=torch.float32, device=self.device)\n",
        "            self.mat_mean = torch.tensor(material_props.mean(axis=0), dtype=torch.float32, device=self.device)\n",
        "            self.mat_std = torch.tensor(material_props.std(axis=0) + 1e-8, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        print(\"📊 Normalization statistics computed\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_points)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get normalized sample\"\"\"\n",
        "        if self.cache_in_memory:\n",
        "            basic_state = self.basic_states[idx]\n",
        "            residual = self.residuals[idx]\n",
        "            material_prop = self.material_props[idx]\n",
        "        else:\n",
        "            # Cargar desde datos originales\n",
        "            dp = self.data_points[idx]\n",
        "            basic_state = torch.tensor(dp.basic_state, dtype=torch.float32, device=self.device)\n",
        "            residual = torch.tensor(dp.residual, dtype=torch.float32, device=self.device)\n",
        "            material_prop = torch.tensor(dp.material_properties, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        if self.normalize:\n",
        "            basic_state = (basic_state - self.state_mean) / self.state_std\n",
        "            material_prop = (material_prop - self.mat_mean) / self.mat_std\n",
        "\n",
        "        # Concatenate state + properties as input features\n",
        "        input_features = torch.cat([basic_state, material_prop])\n",
        "\n",
        "        return input_features, residual\n",
        "\n",
        "    def save_to_disk(self, filepath: str):\n",
        "        \"\"\"Guarda el dataset en disco\"\"\"\n",
        "        data_to_save = {\n",
        "            'data_points': [dp.to_dict() for dp in self.data_points],\n",
        "            'normalization_stats': {\n",
        "                'state_mean': self.state_mean.cpu().numpy() if hasattr(self, 'state_mean') else None,\n",
        "                'state_std': self.state_std.cpu().numpy() if hasattr(self, 'state_std') else None,\n",
        "                'mat_mean': self.mat_mean.cpu().numpy() if hasattr(self, 'mat_mean') else None,\n",
        "                'mat_std': self.mat_std.cpu().numpy() if hasattr(self, 'mat_std') else None,\n",
        "            },\n",
        "            'normalize': self.normalize,\n",
        "            'cache_in_memory': self.cache_in_memory\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(data_to_save, f)\n",
        "        print(f\"💾 Dataset saved to {filepath}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_disk(cls, filepath: str, device: torch.device = None) -> 'PhysicsDataset':\n",
        "        \"\"\"Carga el dataset desde disco\"\"\"\n",
        "        with open(filepath, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        data_points = [PhysicsDataPoint.from_dict(dp_dict) for dp_dict in data['data_points']]\n",
        "\n",
        "        dataset = cls(\n",
        "            data_points=data_points,\n",
        "            device=device or torch.device('cpu'),\n",
        "            normalize=data['normalize'],\n",
        "            cache_in_memory=data['cache_in_memory']\n",
        "        )\n",
        "\n",
        "        # Restaurar estadísticas de normalización si existen\n",
        "        if data['normalization_stats']['state_mean'] is not None:\n",
        "            dataset.state_mean = torch.tensor(data['normalization_stats']['state_mean'],\n",
        "                                            dtype=torch.float32, device=dataset.device)\n",
        "            dataset.state_std = torch.tensor(data['normalization_stats']['state_std'],\n",
        "                                           dtype=torch.float32, device=dataset.device)\n",
        "            dataset.mat_mean = torch.tensor(data['normalization_stats']['mat_mean'],\n",
        "                                          dtype=torch.float32, device=dataset.device)\n",
        "            dataset.mat_std = torch.tensor(data['normalization_stats']['mat_std'],\n",
        "                                         dtype=torch.float32, device=dataset.device)\n",
        "\n",
        "        print(f\"📂 Dataset loaded from {filepath}\")\n",
        "        return dataset\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ANALIZADOR DE DATOS\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetAnalyzer:\n",
        "    \"\"\"\n",
        "    Analizador para entender las características del dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset: PhysicsDataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def analyze_distribution(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analiza la distribución de los datos\"\"\"\n",
        "\n",
        "        # Extraer datos para análisis\n",
        "        basic_states = np.array([dp.basic_state for dp in self.dataset.data_points])\n",
        "        residuals = np.array([dp.residual for dp in self.dataset.data_points])\n",
        "        material_props = np.array([dp.material_properties for dp in self.dataset.data_points])\n",
        "\n",
        "        # Análisis de estados básicos\n",
        "        pos_data = basic_states[:, :3]  # Posiciones\n",
        "        vel_data = basic_states[:, 3:]  # Velocidades\n",
        "\n",
        "        # Análisis de residuales\n",
        "        pos_residuals = residuals[:, :3]\n",
        "        vel_residuals = residuals[:, 3:]\n",
        "\n",
        "        analysis = {\n",
        "            'dataset_size': len(self.dataset.data_points),\n",
        "            'position_stats': {\n",
        "                'mean': np.mean(pos_data, axis=0),\n",
        "                'std': np.std(pos_data, axis=0),\n",
        "                'min': np.min(pos_data, axis=0),\n",
        "                'max': np.max(pos_data, axis=0)\n",
        "            },\n",
        "            'velocity_stats': {\n",
        "                'mean': np.mean(vel_data, axis=0),\n",
        "                'std': np.std(vel_data, axis=0),\n",
        "                'min': np.min(vel_data, axis=0),\n",
        "                'max': np.max(vel_data, axis=0)\n",
        "            },\n",
        "            'position_residual_stats': {\n",
        "                'mean': np.mean(pos_residuals, axis=0),\n",
        "                'std': np.std(pos_residuals, axis=0),\n",
        "                'min': np.min(pos_residuals, axis=0),\n",
        "                'max': np.max(pos_residuals, axis=0)\n",
        "            },\n",
        "            'velocity_residual_stats': {\n",
        "                'mean': np.mean(vel_residuals, axis=0),\n",
        "                'std': np.std(vel_residuals, axis=0),\n",
        "                'min': np.min(vel_residuals, axis=0),\n",
        "                'max': np.max(vel_residuals, axis=0)\n",
        "            },\n",
        "            'material_distribution': self._analyze_materials()\n",
        "        }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _analyze_materials(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analiza la distribución de materiales\"\"\"\n",
        "        materials = {}\n",
        "        floor_materials = {}\n",
        "        shapes = {}\n",
        "\n",
        "        for dp in self.dataset.data_points:\n",
        "            # Material del objeto\n",
        "            obj_material = dp.context.get('material', 'unknown')\n",
        "            materials[obj_material] = materials.get(obj_material, 0) + 1\n",
        "\n",
        "            # Material del suelo\n",
        "            floor_material = dp.context.get('floor_material', 'unknown')\n",
        "            floor_materials[floor_material] = floor_materials.get(floor_material, 0) + 1\n",
        "\n",
        "            # Forma del objeto\n",
        "            shape = dp.context.get('shape', 'unknown')\n",
        "            shapes[shape] = shapes.get(shape, 0) + 1\n",
        "\n",
        "        return {\n",
        "            'object_materials': materials,\n",
        "            'floor_materials': floor_materials,\n",
        "            'shapes': shapes\n",
        "        }\n",
        "\n",
        "    def plot_analysis(self):\n",
        "        \"\"\"Visualiza el análisis del dataset\"\"\"\n",
        "        analysis = self.analyze_distribution()\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # Distribución de posiciones\n",
        "        pos_data = np.array([dp.basic_state[:3] for dp in self.dataset.data_points])\n",
        "        for i, axis_name in enumerate(['X', 'Y', 'Z']):\n",
        "            axes[0, i].hist(pos_data[:, i], bins=50, alpha=0.7, color=['red', 'green', 'blue'][i])\n",
        "            axes[0, i].set_title(f'Position {axis_name} Distribution')\n",
        "            axes[0, i].set_xlabel(f'Position {axis_name}')\n",
        "            axes[0, i].set_ylabel('Frequency')\n",
        "            axes[0, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Distribución de velocidades\n",
        "        vel_data = np.array([dp.basic_state[3:] for dp in self.dataset.data_points])\n",
        "        for i, axis_name in enumerate(['VX', 'VY', 'VZ']):\n",
        "            axes[1, i].hist(vel_data[:, i], bins=50, alpha=0.7, color=['orange', 'purple', 'brown'][i])\n",
        "            axes[1, i].set_title(f'Velocity {axis_name} Distribution')\n",
        "            axes[1, i].set_xlabel(f'Velocity {axis_name}')\n",
        "            axes[1, i].set_ylabel('Frequency')\n",
        "            axes[1, i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Gráfico de distribución de materiales\n",
        "        self._plot_material_distribution(analysis['material_distribution'])\n",
        "\n",
        "    def _plot_material_distribution(self, material_dist: Dict):\n",
        "        \"\"\"Grafica la distribución de materiales\"\"\"\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        # Materiales de objetos\n",
        "        if material_dist['object_materials']:\n",
        "            materials = list(material_dist['object_materials'].keys())\n",
        "            counts = list(material_dist['object_materials'].values())\n",
        "            axes[0].pie(counts, labels=materials, autopct='%1.1f%%')\n",
        "            axes[0].set_title('Object Materials Distribution')\n",
        "\n",
        "        # Materiales de suelo\n",
        "        if material_dist['floor_materials']:\n",
        "            floor_materials = list(material_dist['floor_materials'].keys())\n",
        "            floor_counts = list(material_dist['floor_materials'].values())\n",
        "            axes[1].pie(floor_counts, labels=floor_materials, autopct='%1.1f%%')\n",
        "            axes[1].set_title('Floor Materials Distribution')\n",
        "\n",
        "        # Formas\n",
        "        if material_dist['shapes']:\n",
        "            shapes = list(material_dist['shapes'].keys())\n",
        "            shape_counts = list(material_dist['shapes'].values())\n",
        "            axes[2].pie(shape_counts, labels=shapes, autopct='%1.1f%%')\n",
        "            axes[2].set_title('Shapes Distribution')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 4. GENERADOR DE DATOS SINTÉTICOS\n",
        "# ============================================================================\n",
        "\n",
        "class SyntheticDataGenerator:\n",
        "    \"\"\"\n",
        "    Generador de datos sintéticos para aumentar el dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset: PhysicsDataset):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.analyzer = DatasetAnalyzer(base_dataset)\n",
        "        self.stats = self.analyzer.analyze_distribution()\n",
        "\n",
        "    def generate_synthetic_data(self, num_samples: int,\n",
        "                              noise_level: float = 0.1) -> List[PhysicsDataPoint]:\n",
        "        \"\"\"\n",
        "        Genera datos sintéticos basados en el dataset existente\n",
        "        \"\"\"\n",
        "        synthetic_data = []\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            # Seleccionar un punto base aleatorio\n",
        "            base_idx = random.randint(0, len(self.base_dataset.data_points) - 1)\n",
        "            base_point = self.base_dataset.data_points[base_idx]\n",
        "\n",
        "            # Agregar ruido controlado\n",
        "            noisy_basic_state = self._add_noise(base_point.basic_state, noise_level)\n",
        "            noisy_material_props = self._add_noise(base_point.material_properties, noise_level * 0.1)\n",
        "\n",
        "            # Generar residual sintético (con cierta correlación)\n",
        "            synthetic_residual = self._generate_correlated_residual(\n",
        "                noisy_basic_state, base_point.residual, noise_level\n",
        "            )\n",
        "\n",
        "            # Crear nuevo punto de datos\n",
        "            synthetic_point = PhysicsDataPoint(\n",
        "                basic_state=noisy_basic_state,\n",
        "                ground_truth_state=noisy_basic_state + synthetic_residual,\n",
        "                residual=synthetic_residual,\n",
        "                material_properties=noisy_material_props,\n",
        "                context={**base_point.context, 'synthetic': True}\n",
        "            )\n",
        "\n",
        "            synthetic_data.append(synthetic_point)\n",
        "\n",
        "        return synthetic_data\n",
        "\n",
        "    def _add_noise(self, data: np.ndarray, noise_level: float) -> np.ndarray:\n",
        "        \"\"\"Agrega ruido gaussiano a los datos\"\"\"\n",
        "        noise = np.random.normal(0, noise_level, data.shape)\n",
        "        return data + noise * np.std(data, axis=0 if data.ndim > 1 else None)\n",
        "\n",
        "    def _generate_correlated_residual(self, basic_state: np.ndarray,\n",
        "                                    base_residual: np.ndarray,\n",
        "                                    noise_level: float) -> np.ndarray:\n",
        "        \"\"\"Genera residual sintético con correlación física\"\"\"\n",
        "        # Mantener cierta correlación con el residual base\n",
        "        correlation_factor = 0.7\n",
        "        noise_factor = 1 - correlation_factor\n",
        "\n",
        "        noise = np.random.normal(0, noise_level, base_residual.shape)\n",
        "        synthetic_residual = (correlation_factor * base_residual +\n",
        "                            noise_factor * noise * np.std(base_residual))\n",
        "\n",
        "        return synthetic_residual\n",
        "\n",
        "# ============================================================================\n",
        "# 5. UTILIDADES DE VISUALIZACIÓN\n",
        "# ============================================================================\n",
        "\n",
        "class PhysicsVisualizer:\n",
        "    \"\"\"\n",
        "    Herramientas de visualización para datos de física\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_trajectory_3d(states: List[Dict], title: str = \"3D Trajectory\"):\n",
        "        \"\"\"Visualiza una trayectoria en 3D\"\"\"\n",
        "        positions = np.array([state['position'] for state in states])\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 8))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        # Trayectoria\n",
        "        ax.plot(positions[:, 0], positions[:, 1], positions[:, 2],\n",
        "                'b-', linewidth=2, label='Trajectory')\n",
        "\n",
        "        # Punto inicial\n",
        "        ax.scatter(positions[0, 0], positions[0, 1], positions[0, 2],\n",
        "                  color='green', s=100, label='Start')\n",
        "\n",
        "        # Punto final\n",
        "        ax.scatter(positions[-1, 0], positions[-1, 1], positions[-1, 2],\n",
        "                  color='red', s=100, label='End')\n",
        "\n",
        "        ax.set_xlabel('X')\n",
        "        ax.set_ylabel('Y')\n",
        "        ax.set_zlabel('Z')\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_residual_analysis(dataset: PhysicsDataset):\n",
        "        \"\"\"Analiza y visualiza los residuales\"\"\"\n",
        "        residuals = np.array([dp.residual for dp in dataset.data_points])\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "        # Residuales de posición\n",
        "        pos_residuals = residuals[:, :3]\n",
        "        for i, axis in enumerate(['X', 'Y', 'Z']):\n",
        "            axes[0, i].hist(pos_residuals[:, i], bins=50, alpha=0.7)\n",
        "            axes[0, i].set_title(f'Position Residual {axis}')\n",
        "            axes[0, i].set_xlabel(f'Residual {axis}')\n",
        "            axes[0, i].set_ylabel('Frequency')\n",
        "            axes[0, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Residuales de velocidad\n",
        "        vel_residuals = residuals[:, 3:]\n",
        "        for i, axis in enumerate(['VX', 'VY', 'VZ']):\n",
        "            axes[1, i].hist(vel_residuals[:, i], bins=50, alpha=0.7)\n",
        "            axes[1, i].set_title(f'Velocity Residual {axis}')\n",
        "            axes[1, i].set_xlabel(f'Residual V{axis[1]}')\n",
        "            axes[1, i].set_ylabel('Frequency')\n",
        "            axes[1, i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# MENSAJE DE CONFIRMACIÓN\n",
        "# ============================================================================\n",
        "\n",
        "print(\"✅ Clases adicionales de simulación y datos completadas:\")\n",
        "print(\"  • PhysicsDataPoint (completa) - Punto de datos con serialización\")\n",
        "print(\"  • PhysicsDataset (completa) - Dataset optimizado con cache y persistencia\")\n",
        "print(\"  • DatasetAnalyzer - Analizador de distribuciones de datos\")\n",
        "print(\"  • SyntheticDataGenerator - Generador de datos sintéticos\")\n",
        "print(\"  • PhysicsVisualizer - Herramientas de visualización\")\n",
        "print(\"\\n🔧 Funcionalidades adicionales:\")\n",
        "print(\"  • Guardado y carga de datasets\")\n",
        "print(\"  • Análisis estadístico completo\")\n",
        "print(\"  • Generación de datos sintéticos\")\n",
        "print(\"  • Visualizaciones especializadas\")"
      ],
      "metadata": {
        "id": "goWKhNUFtSqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MAIN PIPELINE - SISTEMA HÍBRIDO DE FÍSICA CON IA\n",
        "# ============================================================================\n",
        "# Este script orquesta todo el flujo del sistema híbrido de física\n",
        "\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importar todas las clases definidas anteriormente\n",
        "# (Asumiendo que están en el mismo notebook o importadas)\n",
        "\n",
        "class HybridPhysicsExperiment:\n",
        "    \"\"\"\n",
        "    Clase principal que orquesta todo el experimento de física híbrida\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.results = {}\n",
        "\n",
        "        # Directorios de trabajo\n",
        "        self.setup_directories()\n",
        "\n",
        "        print(f\"🚀 Iniciando Experimento de Física Híbrida\")\n",
        "        print(f\"📱 Dispositivo: {self.device}\")\n",
        "        print(f\"🔧 Configuración: {config['experiment_name']}\")\n",
        "\n",
        "    def setup_directories(self):\n",
        "        \"\"\"Configura los directorios de trabajo\"\"\"\n",
        "        self.base_dir = Path(self.config.get('output_dir', 'hybrid_physics_output'))\n",
        "        self.base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        self.models_dir = self.base_dir / 'models'\n",
        "        self.data_dir = self.base_dir / 'data'\n",
        "        self.plots_dir = self.base_dir / 'plots'\n",
        "        self.logs_dir = self.base_dir / 'logs'\n",
        "\n",
        "        for dir_path in [self.models_dir, self.data_dir, self.plots_dir, self.logs_dir]:\n",
        "            dir_path.mkdir(exist_ok=True)\n",
        "\n",
        "    def run_full_pipeline(self):\n",
        "        \"\"\"Ejecuta el pipeline completo\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"🎯 EJECUTANDO PIPELINE COMPLETO\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        try:\n",
        "            # Paso 1: Generar datos si no existen\n",
        "            dataset = self.step_1_generate_or_load_data()\n",
        "\n",
        "            # Paso 2: Analizar datos\n",
        "            self.step_2_analyze_data(dataset)\n",
        "\n",
        "            # Paso 3: Preparar datos para entrenamiento\n",
        "            train_loader, val_loader, test_loader = self.step_3_prepare_data_loaders(dataset)\n",
        "\n",
        "            # Paso 4: Crear y entrenar modelo\n",
        "            model, trainer = self.step_4_train_model(train_loader, val_loader)\n",
        "\n",
        "            # Paso 5: Evaluar modelo\n",
        "            self.step_5_evaluate_model(model, test_loader, dataset)\n",
        "\n",
        "            # Paso 6: Crear sistema híbrido y validar\n",
        "            self.step_6_validate_hybrid_system(model, dataset)\n",
        "\n",
        "            # Paso 7: Generar reporte final\n",
        "            self.step_7_generate_final_report()\n",
        "\n",
        "            print(\"\\n🎉 ¡PIPELINE COMPLETADO EXITOSAMENTE!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Error en el pipeline: {e}\")\n",
        "            raise\n",
        "\n",
        "    def step_1_generate_or_load_data(self):\n",
        "        \"\"\"Paso 1: Generar o cargar datos\"\"\"\n",
        "        print(\"\\n📊 PASO 1: Generación/Carga de Datos\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        dataset_path = self.data_dir / 'physics_dataset.pkl'\n",
        "\n",
        "        if dataset_path.exists() and not self.config.get('regenerate_data', False):\n",
        "            print(\"📂 Cargando dataset existente...\")\n",
        "            dataset = PhysicsDataset.load_from_disk(str(dataset_path), self.device)\n",
        "        else:\n",
        "            print(\"🔄 Generando nuevo dataset...\")\n",
        "\n",
        "            # Configurar generador de datos\n",
        "            generator_config = self.config.get('data_generation', {})\n",
        "            num_scenarios = generator_config.get('num_scenarios', 1000)\n",
        "            steps_per_scenario = generator_config.get('steps_per_scenario', 100)\n",
        "\n",
        "            # Generar datos\n",
        "            data_generator = RealPhysicsDataGenerator(\n",
        "                num_scenarios=num_scenarios,\n",
        "                steps_per_scenario=steps_per_scenario\n",
        "            )\n",
        "\n",
        "            physics_dataset = data_generator.generate_dataset()\n",
        "\n",
        "            # Crear dataset PyTorch\n",
        "            dataset = PhysicsDataset(\n",
        "                physics_dataset,\n",
        "                device=self.device,\n",
        "                normalize=True\n",
        "            )\n",
        "\n",
        "            # Guardar dataset\n",
        "            dataset.save_to_disk(str(dataset_path))\n",
        "\n",
        "        print(f\"✅ Dataset listo: {len(dataset)} muestras\")\n",
        "        self.results['dataset_size'] = len(dataset)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def step_2_analyze_data(self, dataset):\n",
        "        \"\"\"Paso 2: Analizar distribución de datos\"\"\"\n",
        "        print(\"\\n🔍 PASO 2: Análisis de Datos\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        analyzer = DatasetAnalyzer(dataset)\n",
        "        analysis = analyzer.analyze_distribution()\n",
        "\n",
        "        # Guardar análisis\n",
        "        analysis_path = self.logs_dir / 'data_analysis.json'\n",
        "        with open(analysis_path, 'w') as f:\n",
        "            # Convertir numpy arrays a listas para JSON\n",
        "            def convert_numpy(obj):\n",
        "                if isinstance(obj, np.ndarray):\n",
        "                    return obj.tolist()\n",
        "                elif isinstance(obj, np.integer):\n",
        "                    return int(obj)\n",
        "                elif isinstance(obj, np.floating):\n",
        "                    return float(obj)\n",
        "                elif isinstance(obj, dict):\n",
        "                    return {key: convert_numpy(value) for key, value in obj.items()}\n",
        "                elif isinstance(obj, list):\n",
        "                    return [convert_numpy(item) for item in obj]\n",
        "                return obj\n",
        "\n",
        "            json.dump(convert_numpy(analysis), f, indent=2)\n",
        "\n",
        "        # Generar visualizaciones\n",
        "        print(\"📈 Generando visualizaciones...\")\n",
        "        analyzer.plot_analysis()\n",
        "        plt.savefig(self.plots_dir / 'data_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        PhysicsVisualizer.plot_residual_analysis(dataset)\n",
        "        plt.savefig(self.plots_dir / 'residual_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"✅ Análisis completado\")\n",
        "        self.results['data_analysis'] = analysis\n",
        "\n",
        "    def step_3_prepare_data_loaders(self, dataset):\n",
        "        \"\"\"Paso 3: Preparar data loaders\"\"\"\n",
        "        print(\"\\n🔄 PASO 3: Preparación de Data Loaders\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Configuración de división de datos\n",
        "        data_config = self.config.get('data_split', {})\n",
        "        train_ratio = data_config.get('train_ratio', 0.7)\n",
        "        val_ratio = data_config.get('val_ratio', 0.2)\n",
        "        test_ratio = data_config.get('test_ratio', 0.1)\n",
        "\n",
        "        # Verificar que las proporciones suman 1\n",
        "        total_ratio = train_ratio + val_ratio + test_ratio\n",
        "        if abs(total_ratio - 1.0) > 1e-6:\n",
        "            raise ValueError(f\"Las proporciones deben sumar 1.0, pero suman {total_ratio}\")\n",
        "\n",
        "        # Calcular tamaños\n",
        "        dataset_size = len(dataset)\n",
        "        train_size = int(train_ratio * dataset_size)\n",
        "        val_size = int(val_ratio * dataset_size)\n",
        "        test_size = dataset_size - train_size - val_size\n",
        "\n",
        "        # Dividir dataset\n",
        "        train_dataset, val_dataset, test_dataset = random_split(\n",
        "            dataset, [train_size, val_size, test_size],\n",
        "            generator=torch.Generator().manual_seed(42)\n",
        "        )\n",
        "\n",
        "        # Configurar data loaders\n",
        "        batch_size = self.config.get('training', {}).get('batch_size', 512)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=0, pin_memory=False\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=0, pin_memory=False\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=0, pin_memory=False\n",
        "        )\n",
        "\n",
        "        print(f\"✅ Data loaders creados:\")\n",
        "        print(f\"  📚 Entrenamiento: {len(train_dataset)} muestras\")\n",
        "        print(f\"  🔍 Validación: {len(val_dataset)} muestras\")\n",
        "        print(f\"  🧪 Prueba: {len(test_dataset)} muestras\")\n",
        "        print(f\"  📦 Batch size: {batch_size}\")\n",
        "\n",
        "        self.results['data_split'] = {\n",
        "            'train_size': len(train_dataset),\n",
        "            'val_size': len(val_dataset),\n",
        "            'test_size': len(test_dataset),\n",
        "            'batch_size': batch_size\n",
        "        }\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "    def step_4_train_model(self, train_loader, val_loader):\n",
        "        \"\"\"Paso 4: Entrenar modelo\"\"\"\n",
        "        print(\"\\n🧠 PASO 4: Entrenamiento del Modelo\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Configuración del modelo\n",
        "        model_config = self.config.get('model', {})\n",
        "\n",
        "        # Determinar dimensiones\n",
        "        sample_input, sample_output = next(iter(train_loader))\n",
        "        input_dim = sample_input.shape[1]\n",
        "        output_dim = sample_output.shape[1]\n",
        "\n",
        "        # Crear modelo\n",
        "        model = HybridPhysicsModel(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=model_config.get('hidden_dims', [1024, 512, 256, 128]),\n",
        "            output_dim=output_dim,\n",
        "            dropout_rate=model_config.get('dropout_rate', 0.2),\n",
        "            activation=model_config.get('activation', 'leakyrelu')\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Configurar entrenamiento\n",
        "        training_config = self.config.get('training', {})\n",
        "\n",
        "        # Optimizador\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=training_config.get('lr', 1e-3),\n",
        "            weight_decay=training_config.get('weight_decay', 1e-4)\n",
        "        )\n",
        "\n",
        "        # Scheduler\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "\n",
        "        # Función de pérdida\n",
        "        loss_config = self.config.get('loss', {})\n",
        "        criterion = PhysicsLoss(\n",
        "            position_weight=loss_config.get('position_weight', 2.0),\n",
        "            velocity_weight=loss_config.get('velocity_weight', 1.0)\n",
        "        )\n",
        "\n",
        "        # Crear entrenador\n",
        "        trainer = PhysicsTrainer(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            device=self.device,\n",
        "            repo_id=f\"hybrid-physics-{self.config['experiment_name']}\",\n",
        "            use_warmup=training_config.get('use_warmup', True),\n",
        "            warmup_epochs=training_config.get('warmup_epochs', 5)\n",
        "        )\n",
        "\n",
        "        # Entrenar\n",
        "        num_epochs = training_config.get('epochs', 30)\n",
        "        early_stopping_patience = training_config.get('early_stopping_patience', 8)\n",
        "\n",
        "        print(f\"🔥 Iniciando entrenamiento por {num_epochs} épocas...\")\n",
        "        trainer.train(num_epochs=num_epochs, early_stopping_patience=early_stopping_patience)\n",
        "\n",
        "        # Guardar modelo\n",
        "        model_path = self.models_dir / 'best_hybrid_model.pth'\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'model_config': {\n",
        "                'input_dim': input_dim,\n",
        "                'hidden_dims': model_config.get('hidden_dims', [1024, 512, 256, 128]),\n",
        "                'output_dim': output_dim,\n",
        "                'dropout_rate': model_config.get('dropout_rate', 0.2),\n",
        "                'activation': model_config.get('activation', 'leakyrelu')\n",
        "            },\n",
        "            'training_history': {\n",
        "                'train_losses': trainer.train_losses,\n",
        "                'val_losses': trainer.val_losses,\n",
        "                'learning_rates': trainer.learning_rates,\n",
        "                'best_val_loss': trainer.best_val_loss\n",
        "            }\n",
        "        }, model_path)\n",
        "\n",
        "        # Visualizar historial de entrenamiento\n",
        "        trainer.plot_training_history()\n",
        "        plt.savefig(self.plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"✅ Entrenamiento completado\")\n",
        "        self.results['training'] = {\n",
        "            'best_val_loss': trainer.best_val_loss,\n",
        "            'final_train_loss': trainer.train_losses[-1] if trainer.train_losses else None,\n",
        "            'total_epochs': len(trainer.train_losses),\n",
        "            'model_parameters': sum(p.numel() for p in model.parameters())\n",
        "        }\n",
        "\n",
        "        return model, trainer\n",
        "\n",
        "    def step_5_evaluate_model(self, model, test_loader, dataset):\n",
        "        \"\"\"Paso 5: Evaluar modelo en conjunto de prueba\"\"\"\n",
        "        print(\"\\n📊 PASO 5: Evaluación del Modelo\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        evaluator = ModelEvaluator(model, self.device)\n",
        "        metrics = evaluator.compute_metrics(test_loader)\n",
        "\n",
        "        print(\"📈 Métricas en conjunto de prueba:\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            print(f\"  {metric_name}: {value:.6f}\")\n",
        "\n",
        "        # Guardar métricas\n",
        "        metrics_path = self.logs_dir / 'test_metrics.json'\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "\n",
        "        self.results['test_metrics'] = metrics\n",
        "        print(\"✅ Evaluación completada\")\n",
        "\n",
        "    def step_6_validate_hybrid_system(self, model, dataset):\n",
        "        \"\"\"Paso 6: Validar sistema híbrido completo\"\"\"\n",
        "        print(\"\\n🔬 PASO 6: Validación del Sistema Híbrido\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Crear sistema híbrido\n",
        "        basic_physics_core = NewtonianPhysics()\n",
        "\n",
        "        normalization_stats = {\n",
        "            'state_mean': dataset.state_mean,\n",
        "            'state_std': dataset.state_std,\n",
        "            'mat_mean': dataset.mat_mean,\n",
        "            'mat_std': dataset.mat_std\n",
        "        }\n",
        "\n",
        "        hybrid_system = RealHybridPhysicsSystem(\n",
        "            trained_model=model,\n",
        "            physics_core=basic_physics_core,\n",
        "            normalization_stats=normalization_stats\n",
        "        )\n",
        "\n",
        "        # Crear simulador de alta fidelidad\n",
        "        high_fidelity_sim = HighFidelitySimulator(gui=False)\n",
        "\n",
        "        # Crear validador\n",
        "        validator = PhysicsValidator(\n",
        "            hybrid_system=hybrid_system,\n",
        "            basic_physics=basic_physics_core,\n",
        "            high_fidelity_sim=high_fidelity_sim\n",
        "        )\n",
        "\n",
        "        # Crear escenarios de prueba\n",
        "        validation_config = self.config.get('validation', {})\n",
        "        num_test_scenarios = validation_config.get('num_scenarios', 5)\n",
        "        num_steps = validation_config.get('num_steps', 50)\n",
        "\n",
        "        print(f\"🧪 Creando {num_test_scenarios} escenarios de prueba...\")\n",
        "        test_scenarios = create_test_scenarios(num_scenarios=num_test_scenarios)\n",
        "\n",
        "        # Ejecutar validación\n",
        "        print(f\"⚡ Ejecutando validación con {num_steps} pasos por escenario...\")\n",
        "        validation_results = validator.run_comparison_test(test_scenarios, num_steps=num_steps)\n",
        "\n",
        "        # Analizar resultados\n",
        "        stats, improvement = validator.analyze_results(validation_results)\n",
        "\n",
        "        # Visualizar comparación para el primer escenario\n",
        "        validator.plot_comparison(validation_results, scenario_idx=0)\n",
        "        plt.savefig(self.plots_dir / 'hybrid_validation.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Guardar resultados de validación\n",
        "        validation_summary = {\n",
        "            'hybrid_stats': stats['hybrid'],\n",
        "            'basic_stats': stats['basic'],\n",
        "            'improvement': improvement,\n",
        "            'num_scenarios': num_test_scenarios,\n",
        "            'steps_per_scenario': num_steps\n",
        "        }\n",
        "\n",
        "        validation_path = self.logs_dir / 'validation_results.json'\n",
        "        with open(validation_path, 'w') as f:\n",
        "            json.dump(validation_summary, f, indent=2)\n",
        "\n",
        "        print(\"✅ Validación del sistema híbrido completada\")\n",
        "        self.results['validation'] = validation_summary\n",
        "\n",
        "    def step_7_generate_final_report(self):\n",
        "        \"\"\"Paso 7: Generar reporte final\"\"\"\n",
        "        print(\"\\n📄 PASO 7: Generación de Reporte Final\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Crear reporte completo\n",
        "        report = self._create_final_report()\n",
        "\n",
        "        # Guardar reporte\n",
        "        report_path = self.base_dir / 'REPORTE_FINAL.md'\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        # Guardar resultados completos en JSON\n",
        "        results_path = self.logs_dir / 'experiment_results.json'\n",
        "        with open(results_path, 'w') as f:\n",
        "            json.dump(self.results, f, indent=2)\n",
        "\n",
        "        print(f\"📊 Reporte final guardado en: {report_path}\")\n",
        "        print(f\"💾 Resultados completos en: {results_path}\")\n",
        "        print(\"✅ Reporte generado exitosamente\")\n",
        "\n",
        "    def _create_final_report(self) -> str:\n",
        "        \"\"\"Crea el reporte final en formato Markdown\"\"\"\n",
        "\n",
        "        validation = self.results.get('validation', {})\n",
        "        training = self.results.get('training', {})\n",
        "\n",
        "        report = f\"\"\"# 🚀 REPORTE FINAL - SISTEMA HÍBRIDO DE FÍSICA CON IA\n",
        "\n",
        "## 📋 Información del Experimento\n",
        "- **Nombre del Experimento**: {self.config['experiment_name']}\n",
        "- **Fecha de Ejecución**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- **Dispositivo**: {self.device}\n",
        "\n",
        "## 📊 Resumen de Datos\n",
        "- **Tamaño del Dataset**: {self.results.get('dataset_size', 'N/A')} muestras\n",
        "- **División de Datos**:\n",
        "  - Entrenamiento: {self.results.get('data_split', {}).get('train_size', 'N/A')} muestras\n",
        "  - Validación: {self.results.get('data_split', {}).get('val_size', 'N/A')} muestras\n",
        "  - Prueba: {self.results.get('data_split', {}).get('test_size', 'N/A')} muestras\n",
        "\n",
        "## 🧠 Configuración del Modelo\n",
        "- **Parámetros del Modelo**: {training.get('model_parameters', 'N/A'):,} parámetros\n",
        "- **Arquitectura**: {self.config.get('model', {}).get('hidden_dims', 'N/A')}\n",
        "- **Función de Activación**: {self.config.get('model', {}).get('activation', 'N/A')}\n",
        "\n",
        "## 📈 Resultados del Entrenamiento\n",
        "- **Mejor Pérdida de Validación**: {training.get('best_val_loss', 'N/A'):.6f}\n",
        "- **Pérdida Final de Entrenamiento**: {training.get('final_train_loss', 'N/A'):.6f}\n",
        "- **Épocas Completadas**: {training.get('total_epochs', 'N/A')}\n",
        "\n",
        "## 🎯 Rendimiento del Sistema Híbrido\n",
        "- **Mejora en Precisión de Posición**: {validation.get('improvement', {}).get('position_error_reduction', 'N/A'):.1f}%\n",
        "- **Mejora en Precisión de Velocidad**: {validation.get('improvement', {}).get('velocity_error_reduction', 'N/A'):.1f}%\n",
        "\n",
        "### Métricas Detalladas:\n",
        "#### Sistema Híbrido:\n",
        "- Error promedio posición: {validation.get('hybrid_stats', {}).get('mean_pos_error', 'N/A'):.6f}\n",
        "- Error promedio velocidad: {validation.get('hybrid_stats', {}).get('mean_vel_error', 'N/A'):.6f}\n",
        "\n",
        "#### Física Básica:\n",
        "- Error promedio posición: {validation.get('basic_stats', {}).get('mean_pos_error', 'N/A'):.6f}\n",
        "- Error promedio velocidad: {validation.get('basic_stats', {}).get('mean_vel_error', 'N/A'):.6f}\n",
        "\n",
        "## 🔬 Innovaciones Técnicas\n",
        "1. **Física Básica Mejorada**: Inclusión de fricción estática/dinámica y resistencia del aire\n",
        "2. **Dataset Realista**: Generación automática con múltiples materiales y condiciones\n",
        "3. **Arquitectura Híbrida**: Combinación óptima de física determinista + corrección por IA\n",
        "4. **Entrenamiento Optimizado**: Mixed precision, warmup, early stopping\n",
        "5. **Validación Rigurosa**: Comparación directa con simulador de alta fidelidad PyBullet\n",
        "\n",
        "## 📁 Archivos Generados\n",
        "- `models/best_hybrid_model.pth` - Modelo entrenado\n",
        "- `data/physics_dataset.pkl` - Dataset generado\n",
        "- `plots/` - Visualizaciones y gráficos\n",
        "- `logs/` - Métricas y análisis detallados\n",
        "\n",
        "## 🚀 Aplicaciones Potenciales\n",
        "- Simulaciones de videojuegos en tiempo real\n",
        "- Sistemas de entrenamiento robótico\n",
        "- Predicción de movimiento de objetos físicos\n",
        "- Simulaciones científicas aceleradas\n",
        "\n",
        "## 📈 Próximos Pasos\n",
        "- Expansión a sistemas multi-objeto\n",
        "- Inclusión de deformaciones y rotaciones complejas\n",
        "- Optimización para inferencia en tiempo real\n",
        "- Integración con motores de física comerciales\n",
        "\n",
        "---\n",
        "*Generado automáticamente por el Sistema Híbrido de Física con IA*\n",
        "\"\"\"\n",
        "        return report\n",
        "\n",
        "\n",
        "def create_default_config() -> Dict[str, Any]:\n",
        "    \"\"\"Crea configuración por defecto para el experimento\"\"\"\n",
        "    return {\n",
        "        'experiment_name': 'hybrid_physics_default',\n",
        "        'output_dir': 'hybrid_physics_output',\n",
        "        'regenerate_data': False,\n",
        "\n",
        "        'data_generation': {\n",
        "            'num_scenarios': 1000,\n",
        "            'steps_per_scenario': 100\n",
        "        },\n",
        "\n",
        "        'data_split': {\n",
        "            'train_ratio': 0.7,\n",
        "            'val_ratio': 0.2,\n",
        "            'test_ratio': 0.1\n",
        "        },\n",
        "\n",
        "        'model': {\n",
        "            'hidden_dims': [1024, 512, 256, 128],\n",
        "            'dropout_rate': 0.2,\n",
        "            'activation': 'leakyrelu'\n",
        "        },\n",
        "\n",
        "        'training': {\n",
        "            'batch_size': 512,\n",
        "            'lr': 1e-3,\n",
        "            'weight_decay': 1e-4,\n",
        "            'epochs': 30,\n",
        "            'early_stopping_patience': 8,\n",
        "            'use_warmup': True,\n",
        "            'warmup_epochs': 5\n",
        "        },\n",
        "\n",
        "        'loss': {\n",
        "            'position_weight': 2.0,\n",
        "            'velocity_weight': 1.0\n",
        "        },\n",
        "\n",
        "        'validation': {\n",
        "            'num_scenarios': 5,\n",
        "            'num_steps': 50\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Función principal\"\"\"\n",
        "    print(\"🌟 SISTEMA HÍBRIDO DE FÍSICA CON IA\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Configuración por defecto\n",
        "    config = create_default_config()\n",
        "\n",
        "    # Crear y ejecutar experimento\n",
        "    experiment = HybridPhysicsExperiment(config)\n",
        "    experiment.run_full_pipeline()\n",
        "\n",
        "    print(\"\\n🎊 ¡EXPERIMENTO COMPLETADO EXITOSAMENTE!\")\n",
        "    print(f\"📂 Resultados guardados en: {experiment.base_dir}\")\n",
        "\n",
        "\n",
        "def main_with_custom_config(config_path: str):\n",
        "    \"\"\"Función principal con configuración personalizada\"\"\"\n",
        "    print(\"🌟 SISTEMA HÍBRIDO DE FÍSICA CON IA (Configuración Personalizada)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Cargar configuración personalizada\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Crear y ejecutar experimento\n",
        "    experiment = HybridPhysicsExperiment(config)\n",
        "    experiment.run_full_pipeline()\n",
        "\n",
        "    print(\"\\n🎊 ¡EXPERIMENTO COMPLETADO EXITOSAMENTE!\")\n",
        "    print(f\"📂 Resultados guardados en: {experiment.base_dir}\")\n",
        "\n",
        "\n",
        "def create_sample_config():\n",
        "    \"\"\"Crea un archivo de configuración de ejemplo\"\"\"\n",
        "    config = create_default_config()\n",
        "\n",
        "    # Modificar para experimento de ejemplo\n",
        "    config['experiment_name'] = 'physics_experiment_sample'\n",
        "    config['data_generation']['num_scenarios'] = 500  # Menos datos para prueba rápida\n",
        "    config['training']['epochs'] = 15  # Menos épocas para prueba rápida\n",
        "\n",
        "    config_path = 'sample_config.json'\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"📄 Configuración de ejemplo creada: {config_path}\")\n",
        "    return config_path\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PUNTO DE ENTRADA PRINCIPAL\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Si se ejecuta directamente, usar configuración por defecto\n",
        "    main()\n",
        "\n",
        "# Para usar en Jupyter Notebook:\n",
        "def run_experiment(custom_config: Optional[Dict] = None):\n",
        "    \"\"\"\n",
        "    Función de conveniencia para ejecutar desde Jupyter Notebook\n",
        "\n",
        "    Args:\n",
        "        custom_config: Configuración personalizada (opcional)\n",
        "    \"\"\"\n",
        "    if custom_config is None:\n",
        "        config = create_default_config()\n",
        "    else:\n",
        "        config = custom_config\n",
        "\n",
        "    experiment = HybridPhysicsExperiment(config)\n",
        "    experiment.run_full_pipeline()\n",
        "\n",
        "    return experiment\n",
        "\n",
        "# ============================================================================\n",
        "# EJEMPLOS DE USO\n",
        "# ============================================================================\n",
        "\n",
        "def example_quick_test():\n",
        "    \"\"\"Ejemplo para prueba rápida\"\"\"\n",
        "    config = create_default_config()\n",
        "    config['experiment_name'] = 'quick_test'\n",
        "    config['data_generation']['num_scenarios'] = 100\n",
        "    config['training']['epochs'] = 10\n",
        "    config['validation']['num_scenarios'] = 3\n",
        "\n",
        "    return run_experiment(config)\n",
        "\n",
        "def example_full_experiment():\n",
        "    \"\"\"Ejemplo para experimento completo\"\"\"\n",
        "    config = create_default_config()\n",
        "    config['experiment_name'] = 'full_physics_experiment'\n",
        "    config['data_generation']['num_scenarios'] = 2000\n",
        "    config['training']['epochs'] = 50\n",
        "\n",
        "    return run_experiment(config)\n",
        "\n",
        "print(\"✅ Main pipeline definido correctamente\")\n",
        "print(\"\\n🎯 Para ejecutar el experimento completo, usa:\")\n",
        "print(\"  • main() - Ejecuta con configuración por defecto\")\n",
        "print(\"  • run_experiment() - Para usar en Jupyter Notebook\")\n",
        "print(\"  • example_quick_test() - Prueba rápida\")\n",
        "print(\"  • example_full_experiment() - Experimento completo\")\n",
        "print(\"\\n📋 Todas las funciones están listas para usar!\")"
      ],
      "metadata": {
        "id": "9HOjXyoLuAwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "n6FAw1fAFKk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}